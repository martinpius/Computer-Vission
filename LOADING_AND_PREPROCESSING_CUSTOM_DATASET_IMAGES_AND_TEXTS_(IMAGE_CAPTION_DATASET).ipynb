{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LOADING AND PREPROCESSING CUSTOM-DATASET-IMAGES AND TEXTS (IMAGE CAPTION DATASET)",
      "provenance": [],
      "authorship_tag": "ABX9TyNYEQWCYLQ3SELiLr8MDQzT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/Computer-Vission/blob/main/LOADING_AND_PREPROCESSING_CUSTOM_DATASET_IMAGES_AND_TEXTS_(IMAGE_CAPTION_DATASET).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lSZGTjLryC7",
        "outputId": "64077062-1c21-4302-c774-4c1685b897ed"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import torch\n",
        "  print(f\">>>> You are on CoLaB with torch version {torch.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\">>>> {type(e)} {e}\\n>>>> please correct {type(e)} and reload your device\")\n",
        "  COLAB = False\n",
        "def time_fmt(t: float = 129.98)->float:\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"hrs: {h} min: {m:>02} sec: {s:>05.2f}\"\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(f\">>>> testing time formating function.....\\n>>>> time elapsed\\t{time_fmt()}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            ">>>> You are on CoLaB with torch version 1.9.0+cu102\n",
            ">>>> testing time formating function.....\n",
            ">>>> time elapsed\thrs: 0 min: 02 sec: 09.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi9Z2RBPtvOB"
      },
      "source": [
        "# In this notebook we are going to learn how to import and preprocess the dataset from \n",
        "# different platform (here being Google drive). For demo we will work with flickr30k dataset.\n",
        "# since flickr30k consists of both images and texts we implement procedures to process the \n",
        "# data in both perspective.\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nehkccjjx1MW"
      },
      "source": [
        "import torch, os, spacy, random, time, datetime\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4bCv7VxytA5"
      },
      "source": [
        "#set the seed for reproducability:\n",
        "seed = 123\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "spacy_eng = spacy.load(\"en\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egcpuW_1zSVK"
      },
      "source": [
        "# A class to build vocabulary\n",
        "class Vocabulary:\n",
        "  def __init__(self, freq_threshold):\n",
        "    '''\n",
        "    We construct a dictionary which key-values as index-tring and vice-versa\n",
        "    to convert the strings to indice and indices back to strings\n",
        "    <UNK> is when the word doesnt bit the frequency threshold limit.\n",
        "    '''\n",
        "    self.freq_threshold = freq_threshold\n",
        "    self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
        "    self.stoi = {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.itos)\n",
        "\n",
        "  @staticmethod\n",
        "  def eng_tokenizer(text):\n",
        "    ''' \n",
        "    we use spacy-tokenizer to tokenize the texts and then change them to lower cases\n",
        "    '''\n",
        "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "  \n",
        "  def build_vocabulary(self, caption_list):\n",
        "    '''\n",
        "    we send in caption list and build a corpus / bag of vocabulary\n",
        "    in every caption we inspect the each word count. If the word occured\n",
        "    more than frequency threshold we assign an index otherwise it will be assigned\n",
        "    to unknown index.\n",
        "    '''\n",
        "    frequencies = {} # a dictionary / place-holder to store the words\n",
        "    idx = 4 # we start at 4 because 0 = PAD, 1 = SOS, 2 = EOS, 3 = UNK\n",
        "    for caption in caption_list:\n",
        "      for word in caption:\n",
        "        if word not in frequencies:\n",
        "          frequencies[word] = 1\n",
        "        else:\n",
        "          frequencies[word] += 1\n",
        "\n",
        "        #here we do the conversion if the criteria is met\n",
        "        if frequencies[word] == self.freq_threshold:\n",
        "          self.stoi[word] = idx\n",
        "          self.itos[idx] = word\n",
        "          idx += 1\n",
        "\n",
        "  def numericalize(self, text):\n",
        "    '''\n",
        "    we actually convert the texts to numerics using this method\n",
        "\n",
        "    '''\n",
        "    tokenized_text = self.eng_tokenizer(text) # get the tokens in lower cases\n",
        "    return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi['<UNK>']\n",
        "            for token in tokenized_text\n",
        "    ]\n",
        "      \n",
        "\n",
        "# a class to Load the data from google drive\n",
        "class Flickr30kData(Dataset):\n",
        "  def __init__(self, root_dir, csv_file, transform = None, freq_threshold = 5):\n",
        "    '''\n",
        "    root_dir == directory to images folder\n",
        "    csv_file == csv file directory for image discription (id, caption)\n",
        "    transform == if we will apply some transformations to images\n",
        "    freq_threshold == frequency threshold to keep most frequent words in captions\n",
        "\n",
        "    '''\n",
        "    self.root_dir = root_dir\n",
        "    self.dfm = pd.read_csv(csv_file, error_bad_lines = False) # we read the csv file from a specified directory\n",
        "    self.transform = transform\n",
        "\n",
        "    # Grab the image id and caption data from the panda dataframe:\n",
        "    self.imgs = self.dfm['image']\n",
        "    self.captions = self.dfm['caption']\n",
        "\n",
        "    #initialize and buil a vocabulary\n",
        "    self.vocab = Vocabulary(freq_threshold) # we use Vocabulary class (to be defined)\n",
        "    self.vocab.build_vocabulary(self.captions.tolist())\n",
        "  \n",
        "  def __len__(self):\n",
        "    ''' \n",
        "    we grasp total number of examples from our data frame to mark the end of our\n",
        "    loop when we load one datapoint after the other\n",
        "\n",
        "    '''\n",
        "    return len(self.dfm)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    '''\n",
        "    this method help to grasp one sample at a time\n",
        "    a single image with a corresponding caption\n",
        "\n",
        "    '''\n",
        "    caption = self.captions[idx] # grab a caption from an image (texts) from image description csv_file\n",
        "    img_id = self.imgs[idx] # grab the image id from an image description csv-file\n",
        "    img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\") # grab an image from the image folder and convert to RGB\n",
        "    # we apply some transformation to the image if needed\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "    \n",
        "    # We pre-process the texts here: captions (change into numeric)\n",
        "    numericalized_caption = [self.vocab.stoi['<SOS>']] # we start at the begining of the sentennce (SOS)\n",
        "    numericalized_caption += self.vocab.numericalize(caption) # change the caption to numeric\n",
        "    numericalized_caption.append(self.vocab.stoi[\"<EOS>\"]) # mark the end of the sentence <EOS>\n",
        "    # convert to a tensor and then return the image with the corresponding caption\n",
        "    return img, torch.tensor(numericalized_caption)\n",
        "\n",
        "# Since every caption is of specified legth, economically we need to pad-the generated \n",
        "# sequences with the maximum length of a sentence on a specified batch.\n",
        "class MyCollate:\n",
        "  def __init__(self, pad_idx):\n",
        "    self.pad_idx = pad_idx \n",
        "\n",
        "  def __call__(self, batch):\n",
        "    images = [item[0].unsqueeze(0) for item in batch] # list of images with an added batch dimension\n",
        "    images = torch.cat(images, dim = 0) # combine images accross the batch dims\n",
        "    targets = [item[1] for item in batch] # grab all captions \n",
        "    targets = pad_sequence(targets, batch_first = False,padding_value = self.pad_idx) # pad every batch with its max len\n",
        "    return images, targets\n",
        "\n",
        "# We finally define our iterator (dataloader method to stream the data during training)\n",
        "def get_loader(images_dir,\n",
        "               csv_dir,\n",
        "               transform,\n",
        "               batch_size = 64,\n",
        "               shuffle = True,\n",
        "               pin_memory = True):\n",
        "  \n",
        "  #instantiate the data-loader, splits the data into batches padded independntly with their max len\n",
        "  my_flickrdata = Flickr30kData(images_dir, csv_dir, transform)\n",
        "  pad_idx = my_flickrdata.vocab.stoi[\"<PAD>\"] # to use in the custom- collate function\n",
        "  loader = DataLoader(dataset = my_flickrdata, \n",
        "                      batch_size = batch_size, \n",
        "                      shuffle = shuffle,\n",
        "                      pin_memory = pin_memory, \n",
        "                      collate_fn = MyCollate(pad_idx = pad_idx))\n",
        "  return loader\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsruWs1zNKLF",
        "outputId": "15dfa209-787f-4c54-e214-675f85c97e28"
      },
      "source": [
        "# testing our codes by loading the data from google drive.\n",
        "\n",
        "mytransform = transforms.Compose([\n",
        "                                transforms.Resize((224,224)),\n",
        "                                transforms.ToTensor(),])\n",
        "\n",
        "\n",
        "tic = time.time()\n",
        "\n",
        "loader = get_loader(images_dir= \"/content/drive/MyDrive/flickr30k_images/flickr8k/images\",\n",
        "                    csv_dir = \"/content/drive/MyDrive/flickr30k_images/flickr8k/captions.txt\",\n",
        "                    transform = mytransform)\n",
        "\n",
        "for idx, (image, caption) in enumerate(loader):\n",
        "  print(f\">>> image_shape: {image.shape}\\tcaption_shape: {caption.shape}\")\n",
        "\n",
        "tok = time.time()\n",
        "\n",
        "print(f\">>>> time elapsed: {tok - tic}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([30, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([27, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([29, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([22, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([29, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([28, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([25, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([21, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([26, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([27, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([23, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([28, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([21, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([22, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([24, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([26, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([29, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([29, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([26, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([23, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([26, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([23, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([28, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([26, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([22, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([28, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([23, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([20, 64])\n",
            ">>> image_shape: torch.Size([64, 3, 224, 224])\tcaption_shape: torch.Size([27, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLLxxYlXT5f4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}