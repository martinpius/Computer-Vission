{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP from scratch in Pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN9wu6zB/Xxf9xHcPTR35YC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/Computer-Vission/blob/main/MLP_from_scratch_in_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XaPyEG_Rb1R",
        "outputId": "6cfe020d-68dc-4671-bd38-696492234dd9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import torch\n",
        "  print(f\">>>> You are in CoLaB with torch version {torch.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\">>>> {type(e)} {e}\\n>>>> please correct {type(e)} and reload\")\n",
        "def time_fmt(t: float = 123.719)->float:\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"hrs: {h} min: {m:>02} sec: {s:>05.2f}\"\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(f\">>>> time formating\\t..................\\n>>>> time elapsed\\t{time_fmt()}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            ">>>> You are in CoLaB with torch version 1.8.1+cu101\n",
            ">>>> time formating\t..................\n",
            ">>>> time elapsed\thrs: 0 min: 02 sec: 03.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j_ulj0uTUDS"
      },
      "source": [
        "#In this notebook we are going to train a simple multi-layer perceptrons on MNIST dataset:\n",
        "#Model bulding and training loop are all created from scatch with pytorch functionalities:\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8EBbQlsT7yr"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.functional as F\n",
        "import time, random, sys\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3FuS24sUwul"
      },
      "source": [
        "#The following class create our 3 layers MLP:\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_dim, num_class):\n",
        "    super(MLP, self).__init__()\n",
        "    self.fc1 = nn.Linear(in_features = input_dim, out_features = 128)\n",
        "    self.fc2 = nn.Linear(in_features = 128, out_features = 64)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc3 = nn.Linear(in_features = 64, out_features = 32)\n",
        "    self.softmax = nn.Softmax()\n",
        "    self.out = nn.Linear(in_features = 32, out_features = num_class)\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    x = self.relu(self.fc1(input_tensor))\n",
        "    x = self.relu(self.fc2(x))\n",
        "    x = self.relu(self.fc3(x))\n",
        "    x = self.softmax(self.out(x))\n",
        "    return x\n",
        "\n",
        "    "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_EypjfwWnCu",
        "outputId": "03e2022d-76bb-4baa-f336-35339080cb72"
      },
      "source": [
        "#Instantiate and testing the class :\n",
        "input_dim = 784\n",
        "num_class = 10\n",
        "rand_data = torch.rand(size = (64, 784))\n",
        "mlp = MLP(input_dim, num_class).to(device = device)\n",
        "print(f\">>> output shape: {mlp(rand_data).shape}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> output shape: torch.Size([64, 10])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vigdPcnOX-eo",
        "outputId": "d6445350-6363-43dc-a940-5d6e99bdb393"
      },
      "source": [
        "#Import the data from torchvision and preprocess it using transform method:\n",
        "batch_size = 128\n",
        "EPOCHS = 10\n",
        "learning_rate = 1e-3\n",
        "train_dfm = datasets.MNIST(root = 'mnist_mlp_train/', train = True, transform = transforms.ToTensor(), download = True)\n",
        "test_dfm = datasets.MNIST(root = \"mnist_mlp_test/\",train = False, transform = transforms.ToTensor(), download = True)\n",
        "train_loader = DataLoader(dataset = train_dfm, batch_size = batch_size, shuffle = True)\n",
        "test_loader = DataLoader(dataset = test_dfm, batch_size = batch_size, shuffle = False)\n",
        "x_train_batch, y_train_batch = next(iter(train_loader))\n",
        "print(f\">>>> x_train_shape: {x_train_batch.shape}\\ty_train_shape: {y_train_batch.shape}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>> x_train_shape: torch.Size([128, 1, 28, 28])\ty_train_shape: torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYzjPAdx39bT"
      },
      "source": [
        "#Get the loss and optimizer's object:\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params = mlp.parameters(), lr = learning_rate)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqus4AEW40kk",
        "outputId": "893d2163-217b-4126-981c-ad3fe850d239"
      },
      "source": [
        "#The training loop \n",
        "tic = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"\\n>>>> training starts for epoch {epoch + 1}\\n>>>> please wait while the model is training...............\")\n",
        "  for idx, (data, target) in enumerate(tqdm(train_loader)):\n",
        "    data = data.to(device = device)\n",
        "    target = target.to(device = device)\n",
        "    data = data.reshape(data.shape[0],-1)\n",
        "    #forward pass\n",
        "    preds = mlp(data)\n",
        "    train_loss = loss(preds, target)\n",
        "    optimizer.zero_grad()\n",
        "    #backward pass\n",
        "    train_loss.backward()\n",
        "    #gradient descent with adam optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "def __valid__(loder, model):\n",
        "  num_examples = 0\n",
        "  num_correct = 0\n",
        "  model.eval()\n",
        "  #No need to compute the gradients again\n",
        "  with torch.no_grad():\n",
        "    for x, y in loder:\n",
        "      x = x.to(device = device)\n",
        "      y = y.to(device = device)\n",
        "      x = x.reshape(x.shape[0], -1)\n",
        "      preds = model(x) #get the predictions\n",
        "      _, predictions = preds.max(1) #Get the maximum prob amoung the class of 10 probs members\n",
        "      num_correct+=(predictions == y).sum() #Total numer of correctly predicted class\n",
        "      num_examples+=predictions.size(0)\n",
        "    acc = (num_correct/num_examples) * 100\n",
        "    print(f\">>>> at batch {idx + 1} of epoch {epoch + 1}\\n>>>> loss: {train_loss:.4f}, accuracy: {acc:4f}\")\n",
        "  model.train()\n",
        "\n",
        "__valid__(train_loader, mlp)\n",
        "__valid__(test_loader, mlp)\n",
        "\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n",
            "  1%|▏         | 7/469 [00:00<00:07, 65.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>> training starts for epoch 1\n",
            ">>>> please wait while the model is training...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:06<00:00, 68.06it/s]\n",
            "  1%|▏         | 6/469 [00:00<00:08, 57.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>> training starts for epoch 2\n",
            ">>>> please wait while the model is training...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:06<00:00, 67.33it/s]\n",
            "  1%|▏         | 7/469 [00:00<00:07, 63.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>> training starts for epoch 3\n",
            ">>>> please wait while the model is training...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:06<00:00, 68.61it/s]\n",
            "  2%|▏         | 8/469 [00:00<00:06, 69.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>> training starts for epoch 4\n",
            ">>>> please wait while the model is training...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:06<00:00, 67.19it/s]\n",
            "  1%|▏         | 7/469 [00:00<00:06, 67.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>> training starts for epoch 5\n",
            ">>>> please wait while the model is training...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:06<00:00, 67.62it/s]\n",
            "  1%|▏         | 7/469 [00:00<00:06, 67.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>> training starts for epoch 6\n",
            ">>>> please wait while the model is training...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:06<00:00, 68.19it/s]\n",
            "  1%|▏         | 7/469 [00:00<00:06, 68.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>> training starts for epoch 7\n",
            ">>>> please wait while the model is training...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:06<00:00, 68.56it/s]\n",
            "  1%|▏         | 7/469 [00:00<00:06, 67.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>> training starts for epoch 8\n",
            ">>>> please wait while the model is training...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:06<00:00, 68.37it/s]\n",
            "  1%|▏         | 7/469 [00:00<00:07, 63.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>> training starts for epoch 9\n",
            ">>>> please wait while the model is training...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:06<00:00, 68.34it/s]\n",
            "  1%|▏         | 7/469 [00:00<00:06, 66.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>> training starts for epoch 10\n",
            ">>>> please wait while the model is training...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:06<00:00, 67.77it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>>> at batch 469 of epoch 10\n",
            ">>>> loss: 1.4902, accuracy: 99.005005\n",
            ">>>> at batch 469 of epoch 10\n",
            ">>>> loss: 1.4902, accuracy: 97.509995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_HoatpD_C7U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}