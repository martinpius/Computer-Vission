{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN from scratch with tf-keras-layer's subclassing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPKGeT9+CGlRDEunF1GzxWH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/Computer-Vission/blob/main/CNN_from_scratch_with_tf_keras_layer's_subclassing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7y21a2ATY79",
        "outputId": "dee75f54-4f6e-457e-ce3f-11e9e8b5c5a4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import tensorflow as tf\n",
        "  print(f\"You are on CoLab with tensorflow version: {tf.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\"{type(e)}: {e}\\n...please load your drive...\")\n",
        "  COLAB = False\n",
        "def time_fmt(t:float = 231.1829)->float:\n",
        "  h = int(t / (60 * 60)/ 60)\n",
        "  m = int(t % (60 * 60)/ 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"{h}: {m:>03}: {s:>05.2f}\"\n",
        "print(f\"....time testing...time testing...time testing...\\ntime elapse: {time_fmt()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "You are on CoLab with tensorflow version: 2.4.1\n",
            "....time testing...time testing...time testing...\n",
            "time elapse: 0: 003: 51.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpHm9MweU1Ql"
      },
      "source": [
        "#We are going to train a cnn model on mnist dataset from scratch:\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk7Sz-o0VUHA"
      },
      "source": [
        "import time, os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FjK3V76VZKm"
      },
      "source": [
        "#Get and reshape the mnist dataset:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV-5O9UlVfIj"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojna92txVoey",
        "outputId": "a0bf751a-c4dc-445b-e8ac-116b98395ec5"
      },
      "source": [
        "print(f\"x_train_shape: {x_train.shape}, x_test_shape: {x_test.shape}\\ny_train_shape: {y_train.shape}, y_test_shape: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_shape: (60000, 28, 28), x_test_shape: (10000, 28, 28)\n",
            "y_train_shape: (60000,), y_test_shape: (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ce4FAy4V7Ps"
      },
      "source": [
        "#Reshape the images and adding the channel dimension:\n",
        "x_train, x_test = x_train.reshape((-1, 28,28,1)), x_test.reshape((-1,28,28,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9RijHhpWQSa"
      },
      "source": [
        "#Convert into numpy and scale into [0,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lQ9fiG1WYD5"
      },
      "source": [
        "x_train, x_test = x_train.astype(np.float32)/255.0, x_test.astype(np.float32)/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP3fAmoSWm-y",
        "outputId": "438358e4-e10d-4ea7-efaa-14389956ac3a"
      },
      "source": [
        "print(f\"x_train_shape: {x_train.shape}, x_test_shape: {x_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_shape: (60000, 28, 28, 1), x_test_shape: (10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LcmgTehWywl"
      },
      "source": [
        "#Building the classes for the labels: (10 classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuOinFefXB5P"
      },
      "source": [
        "y_train, y_test = tf.keras.utils.to_categorical(y_train, num_classes = 10), tf.keras.utils.to_categorical(y_test, num_classes = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjKIAWrbXREy"
      },
      "source": [
        "#Convert to tensorflow datatypes:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcRVGuuNXXpg",
        "outputId": "f7bf9586-8cd8-4086-9110-5c5d10576baf"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "BUFFER = 1024\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.shuffle(BUFFER).batch(batch_size = BATCH_SIZE, drop_remainder = True)\n",
        "validation_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "validation_data = validation_data.shuffle(BUFFER).batch(batch_size = BATCH_SIZE, drop_remainder = True)\n",
        "x_train_batch_sample, y_train_batch_sample = next(iter(train_data))\n",
        "print(f\"x_train_batch_sample_shape: {x_train_batch_sample.shape}, y_train_batch_sample_shape: {y_train_batch_sample.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_batch_sample_shape: (64, 28, 28, 1), y_train_batch_sample_shape: (64, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXNMb4aCY8Y4"
      },
      "source": [
        "#We now build our model using layer subclassing for convolution block:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSs0qkdjZF0o"
      },
      "source": [
        "class ConvBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_filters, kernel = 3, *args, **kwargs):\n",
        "    super(ConvBlock, self).__init__(*args, **kwargs)\n",
        "    self.conv = tf.keras.layers.Conv2D(filters = num_filters, kernel_size = kernel, padding = 'same', activation = 'relu')\n",
        "    self.bn = tf.keras.layers.BatchNormalization()\n",
        "  \n",
        "  def call(self, inputs, training = False):\n",
        "    x = self.conv(inputs)\n",
        "    x = self.bn(x)\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyKSGO47ciO3"
      },
      "source": [
        "#We can build our model as follow:\n",
        "model = tf.keras.models.Sequential(\n",
        "    [ConvBlock(64),\n",
        "     ConvBlock(128),\n",
        "     ConvBlock(256),\n",
        "     ConvBlock(512),\n",
        "     ConvBlock(128),\n",
        "     ConvBlock(64),\n",
        "     tf.keras.layers.Flatten(),\n",
        "     tf.keras.layers.Dense(units = 1024, activation = 'relu'),\n",
        "     tf.keras.layers.Dropout(rate = 0.5),\n",
        "     tf.keras.layers.Dense(units = 512, activation = 'relu'),\n",
        "     tf.keras.layers.Dense(units = 10, activation = 'softmax')]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XYUppSXhVL8"
      },
      "source": [
        "#The training loop from scratch:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35g-0Is-9JNt"
      },
      "source": [
        "loss_obj = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.001)\n",
        "train_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "eval_metric = tf.keras.metrics.CategoricalAccuracy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpydXbM0u3im",
        "outputId": "e58261f0-e674-4bcb-ee29-49225ed09944"
      },
      "source": [
        "tic = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"The train starts at epoch: {epoch + 1}\")\n",
        "\n",
        "  for (step, (x_train_batch, y_train_batch)) in enumerate(train_data):\n",
        "    with tf.GradientTape() as tape:\n",
        "      preds = model(x_train_batch, training = True)\n",
        "      train_loss = loss_obj(y_train_batch,preds)\n",
        "    grads = tape.gradient(train_loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_metric.update_state(y_train_batch, preds)\n",
        "    train_acc = train_metric.result()\n",
        "    train_metric.reset_states()\n",
        "    if step % 200 == 0:\n",
        "      print(f\"epoch: {epoch + 1} : train accuracy is: {float(train_acc):.4f}\")\n",
        "      print(f\"at batch number: {step} : training loss is: {float(train_loss):.4f}\")\n",
        "  \n",
        "  for (step, (x_eval_batch, y_eval_batch)) in enumerate(validation_data):\n",
        "    preds = model(x_eval_batch, training = False)\n",
        "    eval_loss = loss_obj(y_eval_batch, preds)\n",
        "    eval_metric.update_state(y_eval_batch, preds)\n",
        "    eval_acc = eval_metric.result()\n",
        "    eval_metric.reset_states()\n",
        "    if step % 200 == 0:\n",
        "      print(f\"epoch : {epoch + 1}: validation accuracy is: {float(eval_acc):.4f}\")\n",
        "      print(f\"at batch number {step}: validation loss is: {float(eval_loss):.4f}\")\n",
        "toc = time.time()\n",
        "print(f\"\\ntotal time elapsed in seconds: {time_fmt(toc - tic)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The train starts at epoch: 1\n",
            "epoch: 1 : train accuracy is: 0.1719\n",
            "at batch number: 0 : training loss is: 69.7695\n",
            "epoch: 1 : train accuracy is: 0.9531\n",
            "at batch number: 200 : training loss is: 1.4176\n",
            "epoch: 1 : train accuracy is: 0.8906\n",
            "at batch number: 400 : training loss is: 1.3086\n",
            "epoch: 1 : train accuracy is: 0.9062\n",
            "at batch number: 600 : training loss is: 1.1646\n",
            "epoch: 1 : train accuracy is: 0.9531\n",
            "at batch number: 800 : training loss is: 0.2233\n",
            "epoch : 1: validation accuracy is: 0.9688\n",
            "at batch number 0: validation loss is: 0.1420\n",
            "The train starts at epoch: 2\n",
            "epoch: 2 : train accuracy is: 0.9531\n",
            "at batch number: 0 : training loss is: 0.2648\n",
            "epoch: 2 : train accuracy is: 0.9531\n",
            "at batch number: 200 : training loss is: 0.2501\n",
            "epoch: 2 : train accuracy is: 1.0000\n",
            "at batch number: 400 : training loss is: 0.0091\n",
            "epoch: 2 : train accuracy is: 0.9688\n",
            "at batch number: 600 : training loss is: 0.1335\n",
            "epoch: 2 : train accuracy is: 1.0000\n",
            "at batch number: 800 : training loss is: 0.0066\n",
            "epoch : 2: validation accuracy is: 0.9844\n",
            "at batch number 0: validation loss is: 0.1359\n",
            "The train starts at epoch: 3\n",
            "epoch: 3 : train accuracy is: 0.9531\n",
            "at batch number: 0 : training loss is: 0.2938\n",
            "epoch: 3 : train accuracy is: 1.0000\n",
            "at batch number: 200 : training loss is: 0.0074\n",
            "epoch: 3 : train accuracy is: 0.9531\n",
            "at batch number: 400 : training loss is: 0.2466\n",
            "epoch: 3 : train accuracy is: 0.9844\n",
            "at batch number: 600 : training loss is: 0.0204\n",
            "epoch: 3 : train accuracy is: 0.9844\n",
            "at batch number: 800 : training loss is: 0.0814\n",
            "epoch : 3: validation accuracy is: 0.9688\n",
            "at batch number 0: validation loss is: 0.2495\n",
            "The train starts at epoch: 4\n",
            "epoch: 4 : train accuracy is: 1.0000\n",
            "at batch number: 0 : training loss is: 0.0022\n",
            "epoch: 4 : train accuracy is: 0.9688\n",
            "at batch number: 200 : training loss is: 0.1916\n",
            "epoch: 4 : train accuracy is: 0.9688\n",
            "at batch number: 400 : training loss is: 0.1777\n",
            "epoch: 4 : train accuracy is: 0.9688\n",
            "at batch number: 600 : training loss is: 0.0746\n",
            "epoch: 4 : train accuracy is: 0.9688\n",
            "at batch number: 800 : training loss is: 0.0589\n",
            "epoch : 4: validation accuracy is: 0.9844\n",
            "at batch number 0: validation loss is: 0.0545\n",
            "The train starts at epoch: 5\n",
            "epoch: 5 : train accuracy is: 0.9844\n",
            "at batch number: 0 : training loss is: 0.0607\n",
            "epoch: 5 : train accuracy is: 1.0000\n",
            "at batch number: 200 : training loss is: 0.0000\n",
            "epoch: 5 : train accuracy is: 0.9688\n",
            "at batch number: 400 : training loss is: 1.2419\n",
            "epoch: 5 : train accuracy is: 0.9219\n",
            "at batch number: 600 : training loss is: 0.5579\n",
            "epoch: 5 : train accuracy is: 0.9844\n",
            "at batch number: 800 : training loss is: 0.0122\n",
            "epoch : 5: validation accuracy is: 0.9531\n",
            "at batch number 0: validation loss is: 0.1427\n",
            "The train starts at epoch: 6\n",
            "epoch: 6 : train accuracy is: 1.0000\n",
            "at batch number: 0 : training loss is: 0.0071\n",
            "epoch: 6 : train accuracy is: 1.0000\n",
            "at batch number: 200 : training loss is: 0.0003\n",
            "epoch: 6 : train accuracy is: 0.9844\n",
            "at batch number: 400 : training loss is: 0.0206\n",
            "epoch: 6 : train accuracy is: 0.9844\n",
            "at batch number: 600 : training loss is: 0.0797\n",
            "epoch: 6 : train accuracy is: 0.9531\n",
            "at batch number: 800 : training loss is: 0.1217\n",
            "epoch : 6: validation accuracy is: 0.9531\n",
            "at batch number 0: validation loss is: 0.1336\n",
            "The train starts at epoch: 7\n",
            "epoch: 7 : train accuracy is: 0.9844\n",
            "at batch number: 0 : training loss is: 0.0261\n",
            "epoch: 7 : train accuracy is: 0.9844\n",
            "at batch number: 200 : training loss is: 0.1485\n",
            "epoch: 7 : train accuracy is: 0.9844\n",
            "at batch number: 400 : training loss is: 0.1438\n",
            "epoch: 7 : train accuracy is: 0.9844\n",
            "at batch number: 600 : training loss is: 0.0553\n",
            "epoch: 7 : train accuracy is: 1.0000\n",
            "at batch number: 800 : training loss is: 0.0007\n",
            "epoch : 7: validation accuracy is: 0.9688\n",
            "at batch number 0: validation loss is: 0.3125\n",
            "The train starts at epoch: 8\n",
            "epoch: 8 : train accuracy is: 0.9844\n",
            "at batch number: 0 : training loss is: 0.0200\n",
            "epoch: 8 : train accuracy is: 0.9531\n",
            "at batch number: 200 : training loss is: 0.3984\n",
            "epoch: 8 : train accuracy is: 0.9688\n",
            "at batch number: 400 : training loss is: 0.3967\n",
            "epoch: 8 : train accuracy is: 0.9844\n",
            "at batch number: 600 : training loss is: 0.2175\n",
            "epoch: 8 : train accuracy is: 1.0000\n",
            "at batch number: 800 : training loss is: 0.0005\n",
            "epoch : 8: validation accuracy is: 1.0000\n",
            "at batch number 0: validation loss is: 0.0003\n",
            "The train starts at epoch: 9\n",
            "epoch: 9 : train accuracy is: 0.9844\n",
            "at batch number: 0 : training loss is: 0.0750\n",
            "epoch: 9 : train accuracy is: 0.9844\n",
            "at batch number: 200 : training loss is: 0.0534\n",
            "epoch: 9 : train accuracy is: 0.9844\n",
            "at batch number: 400 : training loss is: 0.3309\n",
            "epoch: 9 : train accuracy is: 0.9688\n",
            "at batch number: 600 : training loss is: 0.1415\n",
            "epoch: 9 : train accuracy is: 1.0000\n",
            "at batch number: 800 : training loss is: 0.0001\n",
            "epoch : 9: validation accuracy is: 0.9844\n",
            "at batch number 0: validation loss is: 0.0985\n",
            "The train starts at epoch: 10\n",
            "epoch: 10 : train accuracy is: 1.0000\n",
            "at batch number: 0 : training loss is: 0.0001\n",
            "epoch: 10 : train accuracy is: 1.0000\n",
            "at batch number: 200 : training loss is: 0.0000\n",
            "epoch: 10 : train accuracy is: 0.9844\n",
            "at batch number: 400 : training loss is: 0.0664\n",
            "epoch: 10 : train accuracy is: 1.0000\n",
            "at batch number: 600 : training loss is: 0.0009\n",
            "epoch: 10 : train accuracy is: 1.0000\n",
            "at batch number: 800 : training loss is: 0.0038\n",
            "epoch : 10: validation accuracy is: 0.9844\n",
            "at batch number 0: validation loss is: 0.0154\n",
            "\n",
            "total time elapsed in seconds: 0: 042: 48.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dYDCIz3Bl4V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}