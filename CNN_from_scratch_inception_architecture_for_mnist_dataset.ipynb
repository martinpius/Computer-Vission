{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN from scratch-inception architecture for mnist dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPQMSZpzgYsBVbM82JB6/dA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/Computer-Vission/blob/main/CNN_from_scratch_inception_architecture_for_mnist_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2R5IEuXXupz",
        "outputId": "9c20d999-00a9-4baf-eb8d-b4c83dc55276"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import tensorflow as tf\n",
        "  print(f\"You are on CoLaB with tensorflow version: {tf.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\"{type(e)}: {e}\\n...please load your drive...\")\n",
        "def time_fmt(x:float = 123.1789)->float:\n",
        "  h = int(x / (60 * 60))\n",
        "  m = int(x % (60 * 60) / 60)\n",
        "  s = int(x % 60)\n",
        "  return f\"{h}: {m:>02}: {s:>05.2f}\"\n",
        "print(f\"...time testing...time testing...time testing>>>:\\ntime elapse: {time_fmt()}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "You are on CoLaB with tensorflow version: 2.4.1\n",
            "...time testing...time testing...time testing>>>:\n",
            "time elapse: 0: 02: 03.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K-jveoKZMkU"
      },
      "source": [
        "#In this notbook we are going to train a simple computer vission model with inception archtecture \n",
        "#on mnist dataset:"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gam_1RLxZm2F"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import time, sys, os"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4ZZS0hNZzxf"
      },
      "source": [
        "#We start by loading the mnist data from tensorflow datasets and prepare it for training:"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZP0eGGCZ7VL"
      },
      "source": [
        "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    "
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsvWUmpnaivN"
      },
      "source": [
        "x_train, x_test = x_train.reshape(-1,28,28,1).astype(np.float32)/255.0, x_test.reshape(-1,28,28,1).astype(np.float32)/255.0"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru5MRMGvbr0G"
      },
      "source": [
        "y_train, y_test = tf.keras.utils.to_categorical(y_train, 10), tf.keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F2VOzYMb1Oe"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "BUFFER = len(x_train)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.shuffle(BUFFER).batch(BATCH_SIZE, drop_remainder = True)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_data = test_data.batch(BATCH_SIZE, drop_remainder = True)\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHmDyNCUeDTL"
      },
      "source": [
        "#Now our data is ready for training: Lets build our model:"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAztLQAZeS5A"
      },
      "source": [
        "#Convolutional block:\n",
        "class CNNBLOCK(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_filters, kernel = 3, *args, **kwargs):\n",
        "    super(CNNBLOCK, self).__init__(*args, **kwargs)\n",
        "    self.conv = tf.keras.layers.Conv2D(filters = num_filters, \n",
        "                                       kernel_size = kernel,\n",
        "                                       kernel_initializer = 'random_normal',\n",
        "                                       strides = 1,padding = 'same')\n",
        "    self.bn = tf.keras.layers.BatchNormalization()\n",
        "    self.myact = tf.keras.layers.LeakyReLU(alpha = 0.3)\n",
        "  def call(self, inputs_tensor , training = False):\n",
        "    x = self.conv(inputs_tensor, training = training)\n",
        "    x = self.myact(x)\n",
        "    x = self.bn(x)\n",
        "    return x"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNORZ2GlhIeg"
      },
      "source": [
        "#Residual Block:\n",
        "class ResNetBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_filters, *args, **kwargs):\n",
        "    super(ResNetBlock, self).__init__(*args, **kwargs)\n",
        "    self.block1 = CNNBLOCK(num_filters = num_filters[0], name = 'bl_1')\n",
        "    self.block2 = CNNBLOCK(num_filters = num_filters[1], name = 'bl_2')\n",
        "    self.block3 = CNNBLOCK(num_filters = num_filters[2], name = 'bl_3')\n",
        "    self.max_pool = tf.keras.layers.MaxPooling2D()\n",
        "    self.id_mapping = tf.keras.layers.Conv2D(filters = num_filters[1], kernel_size = 3, padding = 'same',\n",
        "                                             activation = 'relu', kernel_initializer = 'random_normal')\n",
        "  def call(self, inputs_tensor, training = False):\n",
        "    x = self.block1(inputs_tensor, training = training)\n",
        "    x = self.block2(x, training = training)\n",
        "    x = self.block3(x + self.id_mapping(inputs_tensor), training = training)\n",
        "    return self.max_pool(x)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP18Vi9YhMQC"
      },
      "source": [
        "#Now we build a simple resnet model using model subclassing:"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nik0Og_7r3Ab"
      },
      "source": [
        "class ResModel(tf.keras.Model):\n",
        "  def __init__(self, classes = 10, *args, **kwargs):\n",
        "    super(ResModel, self).__init__(*args, **kwargs)\n",
        "    self.resblock1 = ResNetBlock(num_filters = [32,64,128], name = 'resbl_1')\n",
        "    self.resblock2 = ResNetBlock(num_filters = [128,256,512], name = 'resbl_2')\n",
        "    self.resblock3 = ResNetBlock(num_filters = [64,128,128], name = 'resbl_3')\n",
        "    self.glb = tf.keras.layers.Flatten()\n",
        "    self.dense1 = tf.keras.layers.Dense(units = 1024, activation = 'relu',kernel_initializer = 'random_normal')\n",
        "    self.dense2 = tf.keras.layers.Dense(units = 128, activation = 'relu', kernel_initializer = 'random_normal')\n",
        "    self.outputs = tf.keras.layers.Dense(units = 10, name = 'outputs') \n",
        "  def call(self, inputs_tensor ,training = False):\n",
        "    x = self.resblock1(inputs_tensor, training = training)\n",
        "    x = self.resblock2(x, training = training)\n",
        "    x = self.resblock3(x, training = training)\n",
        "    x = self.glb(x)\n",
        "    x = self.dense1(x, training = training)\n",
        "    x = self.dense2(x, training = training)\n",
        "    x = self.outputs(x, training = training)\n",
        "    return x\n"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdNPExPdvcd8"
      },
      "source": [
        "#We can now instatiating our model ready for training:"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj61vkBsviVH",
        "outputId": "0b8b614c-9992-456a-be61-a796a6263a36"
      },
      "source": [
        "model = ResModel()\n",
        "\n",
        "tic = time.time()\n",
        "model.compile(loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True),\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
        "              metrics = ['accuracy'])\n",
        "model.fit(train_data, epochs = 3, verbose = 2)\n",
        "model.evaluate(test_data, verbose = 2)\n",
        "toc = time.time()\n",
        "print(model.summary())\n",
        "print(f\"\\ntime elapse: {time_fmt(toc - tic)}\")\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "468/468 - 37s - loss: 0.1913 - accuracy: 0.9548\n",
            "Epoch 2/3\n",
            "468/468 - 36s - loss: 0.0424 - accuracy: 0.9870\n",
            "Epoch 3/3\n",
            "468/468 - 36s - loss: 0.0286 - accuracy: 0.9910\n",
            "78/78 - 2s - loss: 0.0251 - accuracy: 0.9928\n",
            "Model: \"res_model_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resbl_1 (ResNetBlock)        multiple                  94208     \n",
            "_________________________________________________________________\n",
            "resbl_2 (ResNetBlock)        multiple                  1921664   \n",
            "_________________________________________________________________\n",
            "resbl_3 (ResNetBlock)        multiple                  1107648   \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             multiple                  1180672   \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             multiple                  131200    \n",
            "_________________________________________________________________\n",
            "outputs (Dense)              multiple                  1290      \n",
            "=================================================================\n",
            "Total params: 4,436,682\n",
            "Trainable params: 4,433,802\n",
            "Non-trainable params: 2,880\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "time elapse: 0: 01: 51.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ_2XGCevmpH",
        "outputId": "b5516136-a5f6-405e-c13d-83ba7c6c2a3b"
      },
      "source": [
        "#The training loop:\n",
        "EPOCHS = 30\n",
        "tic = time.time()\n",
        "loss_obj = tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "train_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "val_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"training starts at epoch: {epoch + 1}\\ntraining::>>>please wait....\")\n",
        "  for (step, (x_train_batch, y_train_batch)) in enumerate(train_data):\n",
        "    with tf.GradientTape() as tape:\n",
        "      preds = model(x_train_batch, training = True)\n",
        "      train_loss = loss_obj(y_train_batch, preds)\n",
        "    grads = tape.gradient(train_loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_metric.update_state(y_train_batch, preds)\n",
        "    train_acc = train_metric.result()\n",
        "    if step % 200 == 0:\n",
        "      print(f\"epoch: {epoch + 1}, train accuracy: {float(train_acc):.4f}\")\n",
        "      print(f\"batch number: {step}, train loss: {float(train_loss):.4f}\")\n",
        "  for (step, (x_val_batch, y_val_batch)) in enumerate(test_data):\n",
        "    preds = model(x_val_batch, training = False)\n",
        "    val_loss = loss_obj(y_val_batch, preds)\n",
        "    val_metric.update_state(y_val_batch, preds)\n",
        "    val_acc = val_metric.result()\n",
        "    val_metric.reset_states()\n",
        "    if step % 200 == 0:\n",
        "      print(f\"epoch: {epoch + 1}, validation accuracy: {float(val_acc):.4f}\")\n",
        "      print(f\"batch number: {step}, validation loss: {float(val_loss):.4f}\")\n",
        "toc = time.time()\n",
        "print(model.summary())\n",
        "print(f\"\\ntotal time for training and evaluation: {time_fmt(toc - tic)}\")"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training starts at epoch: 1\n",
            "training::>>>please wait....\n",
            "epoch: 1, train accuracy: 0.9922\n",
            "batch number: 0, train loss: 0.0116\n",
            "epoch: 1, train accuracy: 0.9835\n",
            "batch number: 200, train loss: 0.0410\n",
            "epoch: 1, train accuracy: 0.9858\n",
            "batch number: 400, train loss: 0.0434\n",
            "epoch: 1, validation accuracy: 0.9922\n",
            "batch number: 0, validation loss: 0.0212\n",
            "training starts at epoch: 2\n",
            "training::>>>please wait....\n",
            "epoch: 2, train accuracy: 0.9863\n",
            "batch number: 0, train loss: 0.0222\n",
            "epoch: 2, train accuracy: 0.9877\n",
            "batch number: 200, train loss: 0.0084\n",
            "epoch: 2, train accuracy: 0.9885\n",
            "batch number: 400, train loss: 0.0160\n",
            "epoch: 2, validation accuracy: 0.9922\n",
            "batch number: 0, validation loss: 0.0331\n",
            "training starts at epoch: 3\n",
            "training::>>>please wait....\n",
            "epoch: 3, train accuracy: 0.9889\n",
            "batch number: 0, train loss: 0.0252\n",
            "epoch: 3, train accuracy: 0.9899\n",
            "batch number: 200, train loss: 0.0019\n",
            "epoch: 3, train accuracy: 0.9904\n",
            "batch number: 400, train loss: 0.0296\n",
            "epoch: 3, validation accuracy: 0.9922\n",
            "batch number: 0, validation loss: 0.0355\n",
            "training starts at epoch: 4\n",
            "training::>>>please wait....\n",
            "epoch: 4, train accuracy: 0.9905\n",
            "batch number: 0, train loss: 0.0230\n",
            "epoch: 4, train accuracy: 0.9911\n",
            "batch number: 200, train loss: 0.0088\n",
            "epoch: 4, train accuracy: 0.9915\n",
            "batch number: 400, train loss: 0.0251\n",
            "epoch: 4, validation accuracy: 1.0000\n",
            "batch number: 0, validation loss: 0.0006\n",
            "training starts at epoch: 5\n",
            "training::>>>please wait....\n",
            "epoch: 5, train accuracy: 0.9916\n",
            "batch number: 0, train loss: 0.0021\n",
            "epoch: 5, train accuracy: 0.9919\n",
            "batch number: 200, train loss: 0.0206\n",
            "epoch: 5, train accuracy: 0.9922\n",
            "batch number: 400, train loss: 0.0264\n",
            "epoch: 5, validation accuracy: 0.9766\n",
            "batch number: 0, validation loss: 0.0798\n",
            "training starts at epoch: 6\n",
            "training::>>>please wait....\n",
            "epoch: 6, train accuracy: 0.9923\n",
            "batch number: 0, train loss: 0.0577\n",
            "epoch: 6, train accuracy: 0.9927\n",
            "batch number: 200, train loss: 0.0057\n",
            "epoch: 6, train accuracy: 0.9929\n",
            "batch number: 400, train loss: 0.0030\n",
            "epoch: 6, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.0784\n",
            "training starts at epoch: 7\n",
            "training::>>>please wait....\n",
            "epoch: 7, train accuracy: 0.9929\n",
            "batch number: 0, train loss: 0.0150\n",
            "epoch: 7, train accuracy: 0.9931\n",
            "batch number: 200, train loss: 0.0062\n",
            "epoch: 7, train accuracy: 0.9933\n",
            "batch number: 400, train loss: 0.0008\n",
            "epoch: 7, validation accuracy: 1.0000\n",
            "batch number: 0, validation loss: 0.0041\n",
            "training starts at epoch: 8\n",
            "training::>>>please wait....\n",
            "epoch: 8, train accuracy: 0.9933\n",
            "batch number: 0, train loss: 0.0131\n",
            "epoch: 8, train accuracy: 0.9936\n",
            "batch number: 200, train loss: 0.0012\n",
            "epoch: 8, train accuracy: 0.9938\n",
            "batch number: 400, train loss: 0.0008\n",
            "epoch: 8, validation accuracy: 0.9922\n",
            "batch number: 0, validation loss: 0.0111\n",
            "training starts at epoch: 9\n",
            "training::>>>please wait....\n",
            "epoch: 9, train accuracy: 0.9938\n",
            "batch number: 0, train loss: 0.0024\n",
            "epoch: 9, train accuracy: 0.9939\n",
            "batch number: 200, train loss: 0.0134\n",
            "epoch: 9, train accuracy: 0.9941\n",
            "batch number: 400, train loss: 0.0005\n",
            "epoch: 9, validation accuracy: 1.0000\n",
            "batch number: 0, validation loss: 0.0010\n",
            "training starts at epoch: 10\n",
            "training::>>>please wait....\n",
            "epoch: 10, train accuracy: 0.9941\n",
            "batch number: 0, train loss: 0.0023\n",
            "epoch: 10, train accuracy: 0.9943\n",
            "batch number: 200, train loss: 0.0001\n",
            "epoch: 10, train accuracy: 0.9944\n",
            "batch number: 400, train loss: 0.0006\n",
            "epoch: 10, validation accuracy: 0.9922\n",
            "batch number: 0, validation loss: 0.0188\n",
            "training starts at epoch: 11\n",
            "training::>>>please wait....\n",
            "epoch: 11, train accuracy: 0.9944\n",
            "batch number: 0, train loss: 0.0010\n",
            "epoch: 11, train accuracy: 0.9946\n",
            "batch number: 200, train loss: 0.0017\n",
            "epoch: 11, train accuracy: 0.9947\n",
            "batch number: 400, train loss: 0.0003\n",
            "epoch: 11, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.0294\n",
            "training starts at epoch: 12\n",
            "training::>>>please wait....\n",
            "epoch: 12, train accuracy: 0.9947\n",
            "batch number: 0, train loss: 0.0297\n",
            "epoch: 12, train accuracy: 0.9948\n",
            "batch number: 200, train loss: 0.0577\n",
            "epoch: 12, train accuracy: 0.9949\n",
            "batch number: 400, train loss: 0.0002\n",
            "epoch: 12, validation accuracy: 1.0000\n",
            "batch number: 0, validation loss: 0.0023\n",
            "training starts at epoch: 13\n",
            "training::>>>please wait....\n",
            "epoch: 13, train accuracy: 0.9950\n",
            "batch number: 0, train loss: 0.0005\n",
            "epoch: 13, train accuracy: 0.9951\n",
            "batch number: 200, train loss: 0.0112\n",
            "epoch: 13, train accuracy: 0.9951\n",
            "batch number: 400, train loss: 0.0068\n",
            "epoch: 13, validation accuracy: 0.9922\n",
            "batch number: 0, validation loss: 0.0100\n",
            "training starts at epoch: 14\n",
            "training::>>>please wait....\n",
            "epoch: 14, train accuracy: 0.9952\n",
            "batch number: 0, train loss: 0.0111\n",
            "epoch: 14, train accuracy: 0.9953\n",
            "batch number: 200, train loss: 0.0104\n",
            "epoch: 14, train accuracy: 0.9953\n",
            "batch number: 400, train loss: 0.0038\n",
            "epoch: 14, validation accuracy: 1.0000\n",
            "batch number: 0, validation loss: 0.0016\n",
            "training starts at epoch: 15\n",
            "training::>>>please wait....\n",
            "epoch: 15, train accuracy: 0.9954\n",
            "batch number: 0, train loss: 0.0060\n",
            "epoch: 15, train accuracy: 0.9955\n",
            "batch number: 200, train loss: 0.0053\n",
            "epoch: 15, train accuracy: 0.9955\n",
            "batch number: 400, train loss: 0.0000\n",
            "epoch: 15, validation accuracy: 1.0000\n",
            "batch number: 0, validation loss: 0.0005\n",
            "training starts at epoch: 16\n",
            "training::>>>please wait....\n",
            "epoch: 16, train accuracy: 0.9955\n",
            "batch number: 0, train loss: 0.0001\n",
            "epoch: 16, train accuracy: 0.9956\n",
            "batch number: 200, train loss: 0.0000\n",
            "epoch: 16, train accuracy: 0.9957\n",
            "batch number: 400, train loss: 0.0370\n",
            "epoch: 16, validation accuracy: 0.9922\n",
            "batch number: 0, validation loss: 0.0700\n",
            "training starts at epoch: 17\n",
            "training::>>>please wait....\n",
            "epoch: 17, train accuracy: 0.9957\n",
            "batch number: 0, train loss: 0.0017\n",
            "epoch: 17, train accuracy: 0.9958\n",
            "batch number: 200, train loss: 0.0000\n",
            "epoch: 17, train accuracy: 0.9958\n",
            "batch number: 400, train loss: 0.0002\n",
            "epoch: 17, validation accuracy: 1.0000\n",
            "batch number: 0, validation loss: 0.0030\n",
            "training starts at epoch: 18\n",
            "training::>>>please wait....\n",
            "epoch: 18, train accuracy: 0.9958\n",
            "batch number: 0, train loss: 0.0250\n",
            "epoch: 18, train accuracy: 0.9959\n",
            "batch number: 200, train loss: 0.0000\n",
            "epoch: 18, train accuracy: 0.9960\n",
            "batch number: 400, train loss: 0.0003\n",
            "epoch: 18, validation accuracy: 0.9922\n",
            "batch number: 0, validation loss: 0.0271\n",
            "training starts at epoch: 19\n",
            "training::>>>please wait....\n",
            "epoch: 19, train accuracy: 0.9960\n",
            "batch number: 0, train loss: 0.0003\n",
            "epoch: 19, train accuracy: 0.9960\n",
            "batch number: 200, train loss: 0.0014\n",
            "epoch: 19, train accuracy: 0.9961\n",
            "batch number: 400, train loss: 0.0267\n",
            "epoch: 19, validation accuracy: 0.9922\n",
            "batch number: 0, validation loss: 0.0255\n",
            "training starts at epoch: 20\n",
            "training::>>>please wait....\n",
            "epoch: 20, train accuracy: 0.9961\n",
            "batch number: 0, train loss: 0.0001\n",
            "epoch: 20, train accuracy: 0.9961\n",
            "batch number: 200, train loss: 0.0059\n",
            "epoch: 20, train accuracy: 0.9962\n",
            "batch number: 400, train loss: 0.0001\n",
            "epoch: 20, validation accuracy: 0.9922\n",
            "batch number: 0, validation loss: 0.0375\n",
            "training starts at epoch: 21\n",
            "training::>>>please wait....\n",
            "epoch: 21, train accuracy: 0.9962\n",
            "batch number: 0, train loss: 0.0012\n",
            "epoch: 21, train accuracy: 0.9963\n",
            "batch number: 200, train loss: 0.0000\n",
            "epoch: 21, train accuracy: 0.9963\n",
            "batch number: 400, train loss: 0.0399\n",
            "epoch: 21, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.0429\n",
            "training starts at epoch: 22\n",
            "training::>>>please wait....\n",
            "epoch: 22, train accuracy: 0.9963\n",
            "batch number: 0, train loss: 0.0001\n",
            "epoch: 22, train accuracy: 0.9963\n",
            "batch number: 200, train loss: 0.0015\n",
            "epoch: 22, train accuracy: 0.9964\n",
            "batch number: 400, train loss: 0.0000\n",
            "epoch: 22, validation accuracy: 1.0000\n",
            "batch number: 0, validation loss: 0.0046\n",
            "training starts at epoch: 23\n",
            "training::>>>please wait....\n",
            "epoch: 23, train accuracy: 0.9964\n",
            "batch number: 0, train loss: 0.0064\n",
            "epoch: 23, train accuracy: 0.9965\n",
            "batch number: 200, train loss: 0.0000\n",
            "epoch: 23, train accuracy: 0.9965\n",
            "batch number: 400, train loss: 0.0000\n",
            "epoch: 23, validation accuracy: 1.0000\n",
            "batch number: 0, validation loss: 0.0029\n",
            "training starts at epoch: 24\n",
            "training::>>>please wait....\n",
            "epoch: 24, train accuracy: 0.9965\n",
            "batch number: 0, train loss: 0.0000\n",
            "epoch: 24, train accuracy: 0.9966\n",
            "batch number: 200, train loss: 0.0000\n",
            "epoch: 24, train accuracy: 0.9966\n",
            "batch number: 400, train loss: 0.0007\n",
            "epoch: 24, validation accuracy: 0.9922\n",
            "batch number: 0, validation loss: 0.0110\n",
            "training starts at epoch: 25\n",
            "training::>>>please wait....\n",
            "epoch: 25, train accuracy: 0.9966\n",
            "batch number: 0, train loss: 0.0102\n",
            "epoch: 25, train accuracy: 0.9967\n",
            "batch number: 200, train loss: 0.0000\n",
            "epoch: 25, train accuracy: 0.9967\n",
            "batch number: 400, train loss: 0.0050\n",
            "epoch: 25, validation accuracy: 1.0000\n",
            "batch number: 0, validation loss: 0.0010\n",
            "training starts at epoch: 26\n",
            "training::>>>please wait....\n",
            "epoch: 26, train accuracy: 0.9967\n",
            "batch number: 0, train loss: 0.0000\n",
            "epoch: 26, train accuracy: 0.9967\n",
            "batch number: 200, train loss: 0.0001\n",
            "epoch: 26, train accuracy: 0.9968\n",
            "batch number: 400, train loss: 0.0035\n",
            "epoch: 26, validation accuracy: 0.9922\n",
            "batch number: 0, validation loss: 0.0257\n",
            "training starts at epoch: 27\n",
            "training::>>>please wait....\n",
            "epoch: 27, train accuracy: 0.9968\n",
            "batch number: 0, train loss: 0.0000\n",
            "epoch: 27, train accuracy: 0.9968\n",
            "batch number: 200, train loss: 0.0000\n",
            "epoch: 27, train accuracy: 0.9969\n",
            "batch number: 400, train loss: 0.0009\n",
            "epoch: 27, validation accuracy: 1.0000\n",
            "batch number: 0, validation loss: 0.0012\n",
            "training starts at epoch: 28\n",
            "training::>>>please wait....\n",
            "epoch: 28, train accuracy: 0.9969\n",
            "batch number: 0, train loss: 0.0451\n",
            "epoch: 28, train accuracy: 0.9969\n",
            "batch number: 200, train loss: 0.0002\n",
            "epoch: 28, train accuracy: 0.9969\n",
            "batch number: 400, train loss: 0.0002\n",
            "epoch: 28, validation accuracy: 1.0000\n",
            "batch number: 0, validation loss: 0.0021\n",
            "training starts at epoch: 29\n",
            "training::>>>please wait....\n",
            "epoch: 29, train accuracy: 0.9969\n",
            "batch number: 0, train loss: 0.0001\n",
            "epoch: 29, train accuracy: 0.9970\n",
            "batch number: 200, train loss: 0.0000\n",
            "epoch: 29, train accuracy: 0.9970\n",
            "batch number: 400, train loss: 0.0001\n",
            "epoch: 29, validation accuracy: 1.0000\n",
            "batch number: 0, validation loss: 0.0022\n",
            "training starts at epoch: 30\n",
            "training::>>>please wait....\n",
            "epoch: 30, train accuracy: 0.9970\n",
            "batch number: 0, train loss: 0.0000\n",
            "epoch: 30, train accuracy: 0.9971\n",
            "batch number: 200, train loss: 0.0372\n",
            "epoch: 30, train accuracy: 0.9971\n",
            "batch number: 400, train loss: 0.0001\n",
            "epoch: 30, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.0192\n",
            "Model: \"res_model_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resbl_1 (ResNetBlock)        multiple                  94208     \n",
            "_________________________________________________________________\n",
            "resbl_2 (ResNetBlock)        multiple                  1921664   \n",
            "_________________________________________________________________\n",
            "resbl_3 (ResNetBlock)        multiple                  1107648   \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             multiple                  1180672   \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             multiple                  131200    \n",
            "_________________________________________________________________\n",
            "outputs (Dense)              multiple                  1290      \n",
            "=================================================================\n",
            "Total params: 4,436,682\n",
            "Trainable params: 4,433,802\n",
            "Non-trainable params: 2,880\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "total time for training and evaluation: 0: 25: 13.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF2SwODv2peY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}