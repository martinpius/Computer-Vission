{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Resnet using model subclassing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMdAB+hzkGQENd4KQwXRz/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/Computer-Vission/blob/main/Simple_Resnet_using_model_subclassing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQMERQsvm4OR",
        "outputId": "c1115df3-d1cf-4c2b-d92f-48b2bf9b23ff"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import tensorflow as tf\n",
        "  print(f'You are on CoLaB with tensorflow version: {tf.__version__}')\n",
        "except Exception as e:\n",
        "  print(f\"{type(e)}: {e}\\nplease load your drive...\")\n",
        "def time_fmt(t:float=124.928)->float:\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"{h}: {m:>03}: {s:>05.2f}\"\n",
        "print(f\"display formated time....\\n>>> time elapse: {time_fmt()}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "You are on CoLaB with tensorflow version: 2.4.1\n",
            "display formated time....\n",
            ">>> time elapse: 0: 002: 04.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTz_RCAsv4bp"
      },
      "source": [
        "#In this notebook we are going to construct the residual network using keras layer's subclassing and fit on cifar10 data:"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XcWeDPZwhC7"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time, os\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDjRF53wxbHE"
      },
      "source": [
        "#We start by constructing the convolutional block:\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCGzwzkT4vuL"
      },
      "source": [
        "class ConvBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_filters, kernel = 3, *args, **kwargs):\n",
        "    super(ConvBlock, self).__init__(*args, **kwargs)\n",
        "    self.conv = tf.keras.layers.Conv2D(filters = num_filters,\n",
        "                                       kernel_size = (kernel, kernel), \n",
        "                                       padding = 'same', \n",
        "                                       activation = 'relu')\n",
        "    self.bn = tf.keras.layers.BatchNormalization()\n",
        "  def call(self, inputs, training = False):\n",
        "    x = self.conv(inputs)\n",
        "    x = self.bn(x)\n",
        "    return x "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HneJ65oC6ymt"
      },
      "source": [
        "#The residual block:"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au9yLrFa62nn"
      },
      "source": [
        "class ResNetBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_filters, *args, **kwargs):\n",
        "    super(ResNetBlock, self).__init__(*args, **kwargs)\n",
        "    self.cnn1 = ConvBlock(num_filters = num_filters[0])\n",
        "    self.cnn2 = ConvBlock(num_filters = num_filters[1])\n",
        "    self.cnn3 = ConvBlock(num_filters = num_filters[2])\n",
        "    self.max_pool = tf.keras.layers.MaxPooling2D()\n",
        "    self.id_map = tf.keras.layers.Conv2D(filters = num_filters[1], kernel_size = 3, padding = 'same')\n",
        "  \n",
        "  def call(self, inputs_tensor, training = False):\n",
        "    x = self.cnn1(inputs_tensor, training = training)\n",
        "    x = self.cnn2(x, training = training)\n",
        "    x = self.cnn3(x + self.id_map(inputs_tensor), training = training)\n",
        "    return self.max_pool(x)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YDpvlEBBrpo"
      },
      "source": [
        "#Now we can define the simple residual model block"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOOqd0n5BytZ"
      },
      "source": [
        "class ResModel(tf.keras.models.Model):\n",
        "  def __init__(self, class_labels = 10, *args, **kwargs):\n",
        "    super(ResModel, self).__init__(*args, **kwargs)\n",
        "    self.block1 = ResNetBlock([32,64,128])\n",
        "    self.block2 = ResNetBlock([64,128,256])\n",
        "    self.block3 = ResNetBlock([128,256,512])\n",
        "    self.block4 = ResNetBlock([256,256,128])\n",
        "    self.block5 = ResNetBlock([64,128,256])\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.dense1 = tf.keras.layers.Dense(units = 1024, activation = 'relu')\n",
        "    self.dropout = tf.keras.layers.Dropout(rate = 0.5)\n",
        "    self.dense2 = tf.keras.layers.Dense(units = 512, activation = 'relu')\n",
        "    self.outputs = tf.keras.layers.Dense(units = 10, activation = 'softmax')\n",
        "\n",
        "  def call(self, inputs_tensor, training = False):\n",
        "    x = self.block1(inputs_tensor, training = training)\n",
        "    x = self.block2(x, training = training)\n",
        "    x = self.block3(x, training = training)\n",
        "    x = self.block4(x, training = training)\n",
        "    x = self.block5(x, training = training)\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense1(x, training = training)\n",
        "    x = self.dropout(x)\n",
        "    x = self.dense2(x, training = training)\n",
        "    x = self.outputs(x)\n",
        "    return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmuHA7nPIjZE"
      },
      "source": [
        "#Instantiate the model class:\n",
        "model = ResModel(10)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8v5ULKkI3iD",
        "outputId": "8ad030d8-61b5-4859-82be-ce11029f5dc8"
      },
      "source": [
        "#Get the data from keras:\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "img_rows, img_cols = 32, 32\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
        "    input_shape = (img_rows, img_cols, 3)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJcivLhrJa2f",
        "outputId": "2274afbe-2e36-4324-8272-d983026af8d2"
      },
      "source": [
        "print(f\"x_train_shape: {x_train.shape}, x_test_shape: {x_test.shape}\\ny_train_shape: {y_train.shape}, y_test_shape: {y_test.shape}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_shape: (50000, 32, 32, 3), x_test_shape: (10000, 32, 32, 3)\n",
            "y_train_shape: (50000, 1), y_test_shape: (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI3HBMRyLPe-"
      },
      "source": [
        "y_train, y_test = tf.keras.utils.to_categorical(y_train, num_classes = 10), tf.keras.utils.to_categorical(y_test, num_classes = 10)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQYVNpBaLfLl"
      },
      "source": [
        "#Convert the data into tensorflow data type:"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVLxXswgMbpn",
        "outputId": "0f4198d1-bd38-444e-d1e8-1034c9f128d1"
      },
      "source": [
        "BUFFER_SIZE = len(x_train)\n",
        "BATCH_SIZE = 64\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.shuffle(BUFFER_SIZE).batch(batch_size = BATCH_SIZE, drop_remainder = True)\n",
        "validation_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "validation_data = validation_data.shuffle(BUFFER_SIZE).batch(batch_size = BATCH_SIZE, drop_remainder = True)\n",
        "x_train_sample_batch, y_train_sample_batch = next(iter(train_data))\n",
        "print(f\"x_train_batch_shape: {x_train_sample_batch.shape}, y_train_sample_batch: {y_train_sample_batch.shape}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_batch_shape: (64, 32, 32, 3), y_train_sample_batch: (64, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogxCefmXkzO1"
      },
      "source": [
        "#the training loop from scratch:"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUtU_Mntk3ga",
        "outputId": "9a54b11e-2130-4d19-ba83-51bcb12164bb"
      },
      "source": [
        "EPOCHS = 25\n",
        "tic = time.time()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "loss_obj = tf.keras.losses.CategoricalCrossentropy()\n",
        "train_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "val_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"....training begin for epoch: {epoch+ 1}.....\\n>>>please wait.....\")\n",
        "  for (step, (x_train_batch, y_train_batch)) in enumerate(train_data):\n",
        "    with tf.GradientTape() as tape:\n",
        "      preds = model(x_train_batch, training = True)\n",
        "      train_loss = loss_obj(y_train_batch, preds)\n",
        "    grads = tape.gradient(train_loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_metric.update_state(y_train_batch, preds)\n",
        "    train_acc = train_metric.result()\n",
        "    train_metric.reset_states()\n",
        "    if step % 200 == 0:\n",
        "      print(f\"epoch: {epoch + 1}, train accuracy: {float(train_acc):.4f}\")\n",
        "      print(f\"at batch {step}, train loss: {float(train_loss):.4f}\")\n",
        "  for (step, (x_val_batch, y_val_batch)) in enumerate(validation_data):\n",
        "    preds = model(x_val_batch, training = False)\n",
        "    val_loss = loss_obj(y_val_batch, preds)\n",
        "    val_metric.update_state(y_val_batch, preds)\n",
        "    val_acc = val_metric.result()\n",
        "    val_metric.reset_states()\n",
        "    if step % 200 == 0:\n",
        "      print(f\"epoch: {epoch + 1}, validation accuracy: {float(val_acc):.4f}\")\n",
        "      print(f\"at batch {step}: validation_loss: {float(val_loss):.4f}\")\n",
        "toc = time.time()\n",
        "print(f\"\\n>>>train and evaluation complited in {time_fmt(toc - tic)}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "....training begin for epoch: 1.....\n",
            ">>>please wait.....\n",
            "epoch: 1, train accuracy: 0.1406\n",
            "at batch 0, train loss: 2.5929\n",
            "epoch: 1, train accuracy: 0.3906\n",
            "at batch 200, train loss: 1.4943\n",
            "epoch: 1, train accuracy: 0.6406\n",
            "at batch 400, train loss: 1.0989\n",
            "epoch: 1, train accuracy: 0.6406\n",
            "at batch 600, train loss: 0.9182\n",
            "epoch: 1, validation accuracy: 0.6406\n",
            "at batch 0: validation_loss: 1.1621\n",
            "....training begin for epoch: 2.....\n",
            ">>>please wait.....\n",
            "epoch: 2, train accuracy: 0.6562\n",
            "at batch 0, train loss: 0.8841\n",
            "epoch: 2, train accuracy: 0.7344\n",
            "at batch 200, train loss: 0.8368\n",
            "epoch: 2, train accuracy: 0.7969\n",
            "at batch 400, train loss: 0.6972\n",
            "epoch: 2, train accuracy: 0.6875\n",
            "at batch 600, train loss: 0.8534\n",
            "epoch: 2, validation accuracy: 0.7031\n",
            "at batch 0: validation_loss: 0.9734\n",
            "....training begin for epoch: 3.....\n",
            ">>>please wait.....\n",
            "epoch: 3, train accuracy: 0.7812\n",
            "at batch 0, train loss: 0.6865\n",
            "epoch: 3, train accuracy: 0.7812\n",
            "at batch 200, train loss: 0.5891\n",
            "epoch: 3, train accuracy: 0.7031\n",
            "at batch 400, train loss: 0.8706\n",
            "epoch: 3, train accuracy: 0.8281\n",
            "at batch 600, train loss: 0.5629\n",
            "epoch: 3, validation accuracy: 0.7344\n",
            "at batch 0: validation_loss: 0.8097\n",
            "....training begin for epoch: 4.....\n",
            ">>>please wait.....\n",
            "epoch: 4, train accuracy: 0.7188\n",
            "at batch 0, train loss: 0.7089\n",
            "epoch: 4, train accuracy: 0.7500\n",
            "at batch 200, train loss: 0.6210\n",
            "epoch: 4, train accuracy: 0.8281\n",
            "at batch 400, train loss: 0.6008\n",
            "epoch: 4, train accuracy: 0.7969\n",
            "at batch 600, train loss: 0.6891\n",
            "epoch: 4, validation accuracy: 0.8281\n",
            "at batch 0: validation_loss: 0.5570\n",
            "....training begin for epoch: 5.....\n",
            ">>>please wait.....\n",
            "epoch: 5, train accuracy: 0.8750\n",
            "at batch 0, train loss: 0.4222\n",
            "epoch: 5, train accuracy: 0.7656\n",
            "at batch 200, train loss: 0.8132\n",
            "epoch: 5, train accuracy: 0.7500\n",
            "at batch 400, train loss: 0.6793\n",
            "epoch: 5, train accuracy: 0.8906\n",
            "at batch 600, train loss: 0.4167\n",
            "epoch: 5, validation accuracy: 0.7969\n",
            "at batch 0: validation_loss: 0.6283\n",
            "....training begin for epoch: 6.....\n",
            ">>>please wait.....\n",
            "epoch: 6, train accuracy: 0.9062\n",
            "at batch 0, train loss: 0.3164\n",
            "epoch: 6, train accuracy: 0.8750\n",
            "at batch 200, train loss: 0.3576\n",
            "epoch: 6, train accuracy: 0.9062\n",
            "at batch 400, train loss: 0.3394\n",
            "epoch: 6, train accuracy: 0.8125\n",
            "at batch 600, train loss: 0.3770\n",
            "epoch: 6, validation accuracy: 0.8125\n",
            "at batch 0: validation_loss: 0.5141\n",
            "....training begin for epoch: 7.....\n",
            ">>>please wait.....\n",
            "epoch: 7, train accuracy: 0.8438\n",
            "at batch 0, train loss: 0.4312\n",
            "epoch: 7, train accuracy: 0.8906\n",
            "at batch 200, train loss: 0.3382\n",
            "epoch: 7, train accuracy: 0.8906\n",
            "at batch 400, train loss: 0.2388\n",
            "epoch: 7, train accuracy: 0.8906\n",
            "at batch 600, train loss: 0.3586\n",
            "epoch: 7, validation accuracy: 0.7500\n",
            "at batch 0: validation_loss: 0.9669\n",
            "....training begin for epoch: 8.....\n",
            ">>>please wait.....\n",
            "epoch: 8, train accuracy: 0.9375\n",
            "at batch 0, train loss: 0.2625\n",
            "epoch: 8, train accuracy: 0.8906\n",
            "at batch 200, train loss: 0.4504\n",
            "epoch: 8, train accuracy: 0.9062\n",
            "at batch 400, train loss: 0.3293\n",
            "epoch: 8, train accuracy: 0.8281\n",
            "at batch 600, train loss: 0.5799\n",
            "epoch: 8, validation accuracy: 0.7656\n",
            "at batch 0: validation_loss: 0.5844\n",
            "....training begin for epoch: 9.....\n",
            ">>>please wait.....\n",
            "epoch: 9, train accuracy: 0.9531\n",
            "at batch 0, train loss: 0.2318\n",
            "epoch: 9, train accuracy: 0.9062\n",
            "at batch 200, train loss: 0.3525\n",
            "epoch: 9, train accuracy: 0.8750\n",
            "at batch 400, train loss: 0.3198\n",
            "epoch: 9, train accuracy: 0.9375\n",
            "at batch 600, train loss: 0.2091\n",
            "epoch: 9, validation accuracy: 0.8281\n",
            "at batch 0: validation_loss: 0.7902\n",
            "....training begin for epoch: 10.....\n",
            ">>>please wait.....\n",
            "epoch: 10, train accuracy: 0.9375\n",
            "at batch 0, train loss: 0.1408\n",
            "epoch: 10, train accuracy: 0.9375\n",
            "at batch 200, train loss: 0.1852\n",
            "epoch: 10, train accuracy: 0.9375\n",
            "at batch 400, train loss: 0.1926\n",
            "epoch: 10, train accuracy: 0.9062\n",
            "at batch 600, train loss: 0.3523\n",
            "epoch: 10, validation accuracy: 0.7969\n",
            "at batch 0: validation_loss: 0.7374\n",
            "....training begin for epoch: 11.....\n",
            ">>>please wait.....\n",
            "epoch: 11, train accuracy: 0.9062\n",
            "at batch 0, train loss: 0.2164\n",
            "epoch: 11, train accuracy: 0.9219\n",
            "at batch 200, train loss: 0.2190\n",
            "epoch: 11, train accuracy: 0.9062\n",
            "at batch 400, train loss: 0.2303\n",
            "epoch: 11, train accuracy: 0.8906\n",
            "at batch 600, train loss: 0.3683\n",
            "epoch: 11, validation accuracy: 0.7344\n",
            "at batch 0: validation_loss: 0.7502\n",
            "....training begin for epoch: 12.....\n",
            ">>>please wait.....\n",
            "epoch: 12, train accuracy: 0.9531\n",
            "at batch 0, train loss: 0.1202\n",
            "epoch: 12, train accuracy: 0.9688\n",
            "at batch 200, train loss: 0.1040\n",
            "epoch: 12, train accuracy: 0.9062\n",
            "at batch 400, train loss: 0.3134\n",
            "epoch: 12, train accuracy: 0.9844\n",
            "at batch 600, train loss: 0.1285\n",
            "epoch: 12, validation accuracy: 0.8750\n",
            "at batch 0: validation_loss: 0.4528\n",
            "....training begin for epoch: 13.....\n",
            ">>>please wait.....\n",
            "epoch: 13, train accuracy: 0.9844\n",
            "at batch 0, train loss: 0.0752\n",
            "epoch: 13, train accuracy: 0.9688\n",
            "at batch 200, train loss: 0.0515\n",
            "epoch: 13, train accuracy: 0.9531\n",
            "at batch 400, train loss: 0.1867\n",
            "epoch: 13, train accuracy: 0.9688\n",
            "at batch 600, train loss: 0.1386\n",
            "epoch: 13, validation accuracy: 0.7812\n",
            "at batch 0: validation_loss: 1.0552\n",
            "....training begin for epoch: 14.....\n",
            ">>>please wait.....\n",
            "epoch: 14, train accuracy: 0.9219\n",
            "at batch 0, train loss: 0.1323\n",
            "epoch: 14, train accuracy: 0.9375\n",
            "at batch 200, train loss: 0.1755\n",
            "epoch: 14, train accuracy: 0.9375\n",
            "at batch 400, train loss: 0.1830\n",
            "epoch: 14, train accuracy: 0.9688\n",
            "at batch 600, train loss: 0.0734\n",
            "epoch: 14, validation accuracy: 0.8906\n",
            "at batch 0: validation_loss: 0.5574\n",
            "....training begin for epoch: 15.....\n",
            ">>>please wait.....\n",
            "epoch: 15, train accuracy: 0.9688\n",
            "at batch 0, train loss: 0.0862\n",
            "epoch: 15, train accuracy: 0.9688\n",
            "at batch 200, train loss: 0.1420\n",
            "epoch: 15, train accuracy: 0.9688\n",
            "at batch 400, train loss: 0.1689\n",
            "epoch: 15, train accuracy: 1.0000\n",
            "at batch 600, train loss: 0.0271\n",
            "epoch: 15, validation accuracy: 0.8438\n",
            "at batch 0: validation_loss: 0.6540\n",
            "....training begin for epoch: 16.....\n",
            ">>>please wait.....\n",
            "epoch: 16, train accuracy: 0.9688\n",
            "at batch 0, train loss: 0.0917\n",
            "epoch: 16, train accuracy: 0.9688\n",
            "at batch 200, train loss: 0.0808\n",
            "epoch: 16, train accuracy: 0.9531\n",
            "at batch 400, train loss: 0.2090\n",
            "epoch: 16, train accuracy: 0.9688\n",
            "at batch 600, train loss: 0.0733\n",
            "epoch: 16, validation accuracy: 0.8594\n",
            "at batch 0: validation_loss: 0.5159\n",
            "....training begin for epoch: 17.....\n",
            ">>>please wait.....\n",
            "epoch: 17, train accuracy: 0.9844\n",
            "at batch 0, train loss: 0.0703\n",
            "epoch: 17, train accuracy: 0.9219\n",
            "at batch 200, train loss: 0.1093\n",
            "epoch: 17, train accuracy: 0.9688\n",
            "at batch 400, train loss: 0.0439\n",
            "epoch: 17, train accuracy: 0.9688\n",
            "at batch 600, train loss: 0.0672\n",
            "epoch: 17, validation accuracy: 0.8125\n",
            "at batch 0: validation_loss: 0.5397\n",
            "....training begin for epoch: 18.....\n",
            ">>>please wait.....\n",
            "epoch: 18, train accuracy: 0.9844\n",
            "at batch 0, train loss: 0.0346\n",
            "epoch: 18, train accuracy: 1.0000\n",
            "at batch 200, train loss: 0.0168\n",
            "epoch: 18, train accuracy: 0.9688\n",
            "at batch 400, train loss: 0.0412\n",
            "epoch: 18, train accuracy: 0.9688\n",
            "at batch 600, train loss: 0.0583\n",
            "epoch: 18, validation accuracy: 0.7656\n",
            "at batch 0: validation_loss: 1.2702\n",
            "....training begin for epoch: 19.....\n",
            ">>>please wait.....\n",
            "epoch: 19, train accuracy: 0.9531\n",
            "at batch 0, train loss: 0.1109\n",
            "epoch: 19, train accuracy: 0.9844\n",
            "at batch 200, train loss: 0.0711\n",
            "epoch: 19, train accuracy: 0.9844\n",
            "at batch 400, train loss: 0.0306\n",
            "epoch: 19, train accuracy: 0.9844\n",
            "at batch 600, train loss: 0.0922\n",
            "epoch: 19, validation accuracy: 0.8125\n",
            "at batch 0: validation_loss: 0.6766\n",
            "....training begin for epoch: 20.....\n",
            ">>>please wait.....\n",
            "epoch: 20, train accuracy: 0.9531\n",
            "at batch 0, train loss: 0.0983\n",
            "epoch: 20, train accuracy: 0.9688\n",
            "at batch 200, train loss: 0.0396\n",
            "epoch: 20, train accuracy: 0.9688\n",
            "at batch 400, train loss: 0.0578\n",
            "epoch: 20, train accuracy: 0.9688\n",
            "at batch 600, train loss: 0.0593\n",
            "epoch: 20, validation accuracy: 0.8750\n",
            "at batch 0: validation_loss: 0.4780\n",
            "....training begin for epoch: 21.....\n",
            ">>>please wait.....\n",
            "epoch: 21, train accuracy: 0.9688\n",
            "at batch 0, train loss: 0.1120\n",
            "epoch: 21, train accuracy: 1.0000\n",
            "at batch 200, train loss: 0.0077\n",
            "epoch: 21, train accuracy: 0.9688\n",
            "at batch 400, train loss: 0.0789\n",
            "epoch: 21, train accuracy: 0.9531\n",
            "at batch 600, train loss: 0.1037\n",
            "epoch: 21, validation accuracy: 0.8438\n",
            "at batch 0: validation_loss: 0.8032\n",
            "....training begin for epoch: 22.....\n",
            ">>>please wait.....\n",
            "epoch: 22, train accuracy: 0.9688\n",
            "at batch 0, train loss: 0.0707\n",
            "epoch: 22, train accuracy: 0.9844\n",
            "at batch 200, train loss: 0.0359\n",
            "epoch: 22, train accuracy: 1.0000\n",
            "at batch 400, train loss: 0.0021\n",
            "epoch: 22, train accuracy: 0.9844\n",
            "at batch 600, train loss: 0.0292\n",
            "epoch: 22, validation accuracy: 0.7969\n",
            "at batch 0: validation_loss: 0.8323\n",
            "....training begin for epoch: 23.....\n",
            ">>>please wait.....\n",
            "epoch: 23, train accuracy: 0.9688\n",
            "at batch 0, train loss: 0.0638\n",
            "epoch: 23, train accuracy: 0.9844\n",
            "at batch 200, train loss: 0.0322\n",
            "epoch: 23, train accuracy: 0.9844\n",
            "at batch 400, train loss: 0.0232\n",
            "epoch: 23, train accuracy: 0.9844\n",
            "at batch 600, train loss: 0.0402\n",
            "epoch: 23, validation accuracy: 0.8125\n",
            "at batch 0: validation_loss: 1.1268\n",
            "....training begin for epoch: 24.....\n",
            ">>>please wait.....\n",
            "epoch: 24, train accuracy: 1.0000\n",
            "at batch 0, train loss: 0.0203\n",
            "epoch: 24, train accuracy: 1.0000\n",
            "at batch 200, train loss: 0.0071\n",
            "epoch: 24, train accuracy: 1.0000\n",
            "at batch 400, train loss: 0.0178\n",
            "epoch: 24, train accuracy: 0.9531\n",
            "at batch 600, train loss: 0.1416\n",
            "epoch: 24, validation accuracy: 0.9219\n",
            "at batch 0: validation_loss: 0.4051\n",
            "....training begin for epoch: 25.....\n",
            ">>>please wait.....\n",
            "epoch: 25, train accuracy: 1.0000\n",
            "at batch 0, train loss: 0.0280\n",
            "epoch: 25, train accuracy: 0.9688\n",
            "at batch 200, train loss: 0.1058\n",
            "epoch: 25, train accuracy: 0.9844\n",
            "at batch 400, train loss: 0.0916\n",
            "epoch: 25, train accuracy: 0.9844\n",
            "at batch 600, train loss: 0.0508\n",
            "epoch: 25, validation accuracy: 0.8594\n",
            "at batch 0: validation_loss: 0.5791\n",
            "\n",
            ">>>train and evaluation complited in 0: 022: 23.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJq4VK8TpgU7"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}