{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model subclassing-Inception-V3-sample.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPrqeU/v9aeB7V1bMN0cpeg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/Computer-Vission/blob/main/Model_subclassing_Inception_V3_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFmOYxBQyHFq",
        "outputId": "8596df72-eb1c-4baf-a40d-d69baa5ad737"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import tensorflow as tf\n",
        "  print(f\"You are using CoLab with tensorflow version: {tf.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\"{type(e)}: {e}\\n...please load your drive...\")\n",
        "  COLAB = False\n",
        "def time_fmt(x:float = 123.8918)->float:\n",
        "  h = int(x /(60 * 60))\n",
        "  m = int(x % (60 * 60) / 60)\n",
        "  s = int(x % 60)\n",
        "  return f\"{h}: {m:>02}: {s:>05.2f}\"\n",
        "print(f\"...time testing...time testing...time testing...\\n>>>time elapse: {time_fmt()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "You are using CoLab with tensorflow version: 2.4.1\n",
            "...time testing...time testing...time testing...\n",
            ">>>time elapse: 0: 02: 03.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58nGYywR0GnV"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import time, os\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrLRSszJ0r7b"
      },
      "source": [
        "#Convolutional block using layer subclassing:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC5uxLNz1GAC"
      },
      "source": [
        "class ConvBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_filters, kernels = 3, *args, **kwargs):\n",
        "    super(ConvBlock, self).__init__(*args, **kwargs)\n",
        "    self.conv = tf.keras.layers.Conv2D(filters = num_filters, kernel_size = kernels,\n",
        "                                       kernel_initializer = 'random_normal', padding = 'same')\n",
        "    self.bn = tf.keras.layers.BatchNormalization()\n",
        "    self.act = tf.keras.layers.LeakyReLU(alpha = 0.3)\n",
        "\n",
        "  def call(self, inputs_tensor, training = False):\n",
        "    x = self.conv(inputs_tensor, training = training)\n",
        "    x = self.act(x)\n",
        "    x = self.bn(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGVsLFqz2c_z"
      },
      "source": [
        "#Inception Block using layer subclassing:\n",
        "class InceptionBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_filters, *args, **kwargs):\n",
        "    super(InceptionBlock, self).__init__(*args, **kwargs)\n",
        "    self.block1 = ConvBlock(num_filters = num_filters[0])\n",
        "    self.block2 = ConvBlock(num_filters = num_filters[1])\n",
        "    self.block3 = ConvBlock(num_filters = num_filters[2])\n",
        "    self.block4 = ConvBlock(num_filters = num_filters[3])\n",
        "    self.max_pool = tf.keras.layers.MaxPooling2D()\n",
        "    self.id_mapping = tf.keras.layers.Conv2D(filters = num_filters[1], kernel_size = 3, padding = 'same',activation = 'relu')\n",
        "\n",
        "  def call(self, inputs_tensor, training = False):\n",
        "    x = self.block1(inputs_tensor, training = training)\n",
        "    x = self.block2(x, training = training)\n",
        "    x = self.block3(x, training = training)\n",
        "    x = self.block4(x + self.id_mapping(inputs_tensor), training = training)\n",
        "    x = self.max_pool(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybLQG9h7-Bz3"
      },
      "source": [
        "#The model class using model subclassing:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5oaF2FC-PfI"
      },
      "source": [
        "class ModelBlock(tf.keras.models.Model):\n",
        "  def __init__(self, num_classes = 10, *args,**kwargs):\n",
        "    super(ModelBlock, self).__init__(*args, **kwargs)\n",
        "    self.resblock1 = InceptionBlock(num_filters = [32,64,64,128])\n",
        "    self.resblock2 = InceptionBlock(num_filters = [64,128,128,256])\n",
        "    self.resblock3 = InceptionBlock(num_filters = [128,256,256,512])\n",
        "    self.glb = tf.keras.layers.GlobalAveragePooling2D()\n",
        "    self.dense1 = tf.keras.layers.Dense(units = 1024, activation = 'relu', kernel_initializer = 'random_normal')\n",
        "    self.drp = tf.keras.layers.Dropout(rate = 0.25)\n",
        "    self.dense2 = tf.keras.layers.Dense(units = 512, activation = 'relu', kernel_initializer = 'random_normal')\n",
        "    self.out = tf.keras.layers.Dense(units = 10, activation = 'softmax')\n",
        "  \n",
        "  def call(self, inputs_tensor, training = False):\n",
        "    x = self.resblock1(inputs_tensor, training = training)\n",
        "    x = self.resblock2(x, training = training)\n",
        "    x = self.resblock3(x, training = training)\n",
        "    x = self.glb(x)\n",
        "    x = self.dense1(x, training = training)\n",
        "    x = self.drp(x)\n",
        "    x = self.dense2(x, training = training)\n",
        "    x = self.out(x, training = training)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fG5eErKDgFk"
      },
      "source": [
        "#Instatiating the model class and training the model on cifar10 dataset:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXiLg9ziDy2r"
      },
      "source": [
        "model = ModelBlock() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH-yoD8uMiXW"
      },
      "source": [
        "#Loading and preprocess cifar10 dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnihN2f2LwOE"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "img_rows, img_cols = 32, 32\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
        "    input_shape = (3, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
        "    input_shape = (img_rows, img_cols, 3)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train, y_test = tf.keras.utils.to_categorical(y_train, num_classes = 10), tf.keras.utils.to_categorical(y_test, num_classes = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFs8Cb7sNO9C",
        "outputId": "603a10c0-6cb2-42c8-c10e-92424e1d8c6f"
      },
      "source": [
        "print(f\"x_train_shape: {x_train.shape}, x_test_shape: {x_test.shape}\\ny_train_shape: {y_train.shape}, y_test_shape: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_shape: (50000, 32, 32, 3), x_test_shape: (10000, 32, 32, 3)\n",
            "y_train_shape: (50000, 10), y_test_shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCjELWQPNk6M"
      },
      "source": [
        "#Converting into tensorflow datatype:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5R3K4g3NrLc",
        "outputId": "afa69f94-089c-405c-c3e0-472306c26d04"
      },
      "source": [
        "BUFFER = len(x_train)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.shuffle(BUFFER).batch(batch_size = BATCH_SIZE, drop_remainder = True)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_data = test_data.batch(batch_size = BATCH_SIZE, drop_remainder = True)\n",
        "x_train_sample_batch, y_train_sample_batch = next(iter(train_data))\n",
        "print(f\"x_train_batch_shape: {x_train_sample_batch.shape}, y_train_batch_shape: {y_train_sample_batch.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_batch_shape: (64, 32, 32, 3), y_train_batch_shape: (64, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qi03-hiPGhR"
      },
      "source": [
        "#The training loop:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4HAcJ3PPK2s",
        "outputId": "f425ae17-7e9a-446a-96b3-52b8ca52355a"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
        "loss_obj = tf.keras.losses.CategoricalCrossentropy()\n",
        "train_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "eval_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "tic = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"....start of epoch: {epoch + 1}\\n>>>>please wait....\")\n",
        "  for (step, (x_train_batch, y_train_batch)) in enumerate(train_data):\n",
        "    with tf.GradientTape() as tape:\n",
        "      preds = model(x_train_batch, training = True)\n",
        "      train_loss = loss_obj(y_train_batch, preds)\n",
        "    grads = tape.gradient(train_loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_metric.update_state(y_train_batch, preds)\n",
        "    train_acc = train_metric.result()\n",
        "    train_metric.reset_states()\n",
        "    if step % 200 == 0:\n",
        "      print(f\"epoch: {epoch + 1}, train accuracy: {float(train_acc):.4f}\")\n",
        "      print(f\"batch: {step}, train loss: {float(train_loss):.4f}\")\n",
        "  for (step, (x_val_batch, y_val_batch)) in enumerate(test_data):\n",
        "    preds = model(x_val_batch, training = False)\n",
        "    eval_loss = loss_obj(y_val_batch, preds)\n",
        "    eval_metric.update_state(y_val_batch, preds)\n",
        "    val_acc = eval_metric.result()\n",
        "    eval_metric.reset_states()\n",
        "    if step % 200 == 0:\n",
        "      print(f\"epoch: {epoch + 1}, validation accuracy: {float(val_acc):.4f}\")\n",
        "      print(f\"batch: {step}, validation loss: {float(eval_loss):.4f}\")\n",
        "toc = time.time()\n",
        "print(f\"\\n>>> time elaplse for training and evaluation is: {time_fmt(toc- tic)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "....start of epoch: 1\n",
            ">>>>please wait....\n",
            "epoch: 1, train accuracy: 0.6875\n",
            "batch: 0, train loss: 0.9282\n",
            "epoch: 1, train accuracy: 0.6250\n",
            "batch: 200, train loss: 0.9329\n",
            "epoch: 1, train accuracy: 0.5625\n",
            "batch: 400, train loss: 1.2059\n",
            "epoch: 1, train accuracy: 0.7969\n",
            "batch: 600, train loss: 0.6342\n",
            "epoch: 1, validation accuracy: 0.6719\n",
            "batch: 0, validation loss: 1.0611\n",
            "....start of epoch: 2\n",
            ">>>>please wait....\n",
            "epoch: 2, train accuracy: 0.6875\n",
            "batch: 0, train loss: 0.9048\n",
            "epoch: 2, train accuracy: 0.7969\n",
            "batch: 200, train loss: 0.6250\n",
            "epoch: 2, train accuracy: 0.7344\n",
            "batch: 400, train loss: 0.7291\n",
            "epoch: 2, train accuracy: 0.8281\n",
            "batch: 600, train loss: 0.4422\n",
            "epoch: 2, validation accuracy: 0.7500\n",
            "batch: 0, validation loss: 0.7284\n",
            "....start of epoch: 3\n",
            ">>>>please wait....\n",
            "epoch: 3, train accuracy: 0.8125\n",
            "batch: 0, train loss: 0.5096\n",
            "epoch: 3, train accuracy: 0.8438\n",
            "batch: 200, train loss: 0.4096\n",
            "epoch: 3, train accuracy: 0.8906\n",
            "batch: 400, train loss: 0.3837\n",
            "epoch: 3, train accuracy: 0.8750\n",
            "batch: 600, train loss: 0.3526\n",
            "epoch: 3, validation accuracy: 0.7344\n",
            "batch: 0, validation loss: 0.6540\n",
            "....start of epoch: 4\n",
            ">>>>please wait....\n",
            "epoch: 4, train accuracy: 0.8750\n",
            "batch: 0, train loss: 0.3848\n",
            "epoch: 4, train accuracy: 0.8438\n",
            "batch: 200, train loss: 0.4247\n",
            "epoch: 4, train accuracy: 0.8281\n",
            "batch: 400, train loss: 0.4870\n",
            "epoch: 4, train accuracy: 0.8594\n",
            "batch: 600, train loss: 0.4539\n",
            "epoch: 4, validation accuracy: 0.8281\n",
            "batch: 0, validation loss: 0.6063\n",
            "....start of epoch: 5\n",
            ">>>>please wait....\n",
            "epoch: 5, train accuracy: 0.8594\n",
            "batch: 0, train loss: 0.3800\n",
            "epoch: 5, train accuracy: 0.8281\n",
            "batch: 200, train loss: 0.5611\n",
            "epoch: 5, train accuracy: 0.9062\n",
            "batch: 400, train loss: 0.2851\n",
            "epoch: 5, train accuracy: 0.9062\n",
            "batch: 600, train loss: 0.3089\n",
            "epoch: 5, validation accuracy: 0.7812\n",
            "batch: 0, validation loss: 0.8291\n",
            "....start of epoch: 6\n",
            ">>>>please wait....\n",
            "epoch: 6, train accuracy: 0.9375\n",
            "batch: 0, train loss: 0.2255\n",
            "epoch: 6, train accuracy: 0.9062\n",
            "batch: 200, train loss: 0.3189\n",
            "epoch: 6, train accuracy: 0.8906\n",
            "batch: 400, train loss: 0.2750\n",
            "epoch: 6, train accuracy: 0.9688\n",
            "batch: 600, train loss: 0.1554\n",
            "epoch: 6, validation accuracy: 0.8438\n",
            "batch: 0, validation loss: 0.5487\n",
            "....start of epoch: 7\n",
            ">>>>please wait....\n",
            "epoch: 7, train accuracy: 0.9219\n",
            "batch: 0, train loss: 0.2078\n",
            "epoch: 7, train accuracy: 0.9219\n",
            "batch: 200, train loss: 0.1986\n",
            "epoch: 7, train accuracy: 0.9062\n",
            "batch: 400, train loss: 0.3263\n",
            "epoch: 7, train accuracy: 0.8906\n",
            "batch: 600, train loss: 0.2358\n",
            "epoch: 7, validation accuracy: 0.8438\n",
            "batch: 0, validation loss: 0.6178\n",
            "....start of epoch: 8\n",
            ">>>>please wait....\n",
            "epoch: 8, train accuracy: 0.9844\n",
            "batch: 0, train loss: 0.0530\n",
            "epoch: 8, train accuracy: 0.9219\n",
            "batch: 200, train loss: 0.2659\n",
            "epoch: 8, train accuracy: 0.9688\n",
            "batch: 400, train loss: 0.1171\n",
            "epoch: 8, train accuracy: 0.9375\n",
            "batch: 600, train loss: 0.2662\n",
            "epoch: 8, validation accuracy: 0.8125\n",
            "batch: 0, validation loss: 0.7352\n",
            "....start of epoch: 9\n",
            ">>>>please wait....\n",
            "epoch: 9, train accuracy: 0.9844\n",
            "batch: 0, train loss: 0.0764\n",
            "epoch: 9, train accuracy: 0.9688\n",
            "batch: 200, train loss: 0.1478\n",
            "epoch: 9, train accuracy: 0.8906\n",
            "batch: 400, train loss: 0.3674\n",
            "epoch: 9, train accuracy: 0.8906\n",
            "batch: 600, train loss: 0.3190\n",
            "epoch: 9, validation accuracy: 0.8281\n",
            "batch: 0, validation loss: 0.7145\n",
            "....start of epoch: 10\n",
            ">>>>please wait....\n",
            "epoch: 10, train accuracy: 0.9688\n",
            "batch: 0, train loss: 0.0566\n",
            "epoch: 10, train accuracy: 0.9688\n",
            "batch: 200, train loss: 0.0716\n",
            "epoch: 10, train accuracy: 1.0000\n",
            "batch: 400, train loss: 0.0486\n",
            "epoch: 10, train accuracy: 0.9688\n",
            "batch: 600, train loss: 0.1622\n",
            "epoch: 10, validation accuracy: 0.8125\n",
            "batch: 0, validation loss: 0.7694\n",
            "....start of epoch: 11\n",
            ">>>>please wait....\n",
            "epoch: 11, train accuracy: 0.9219\n",
            "batch: 0, train loss: 0.2570\n",
            "epoch: 11, train accuracy: 0.9375\n",
            "batch: 200, train loss: 0.1413\n",
            "epoch: 11, train accuracy: 0.9375\n",
            "batch: 400, train loss: 0.1255\n",
            "epoch: 11, train accuracy: 0.9844\n",
            "batch: 600, train loss: 0.0644\n",
            "epoch: 11, validation accuracy: 0.8438\n",
            "batch: 0, validation loss: 0.5592\n",
            "....start of epoch: 12\n",
            ">>>>please wait....\n",
            "epoch: 12, train accuracy: 0.9844\n",
            "batch: 0, train loss: 0.0715\n",
            "epoch: 12, train accuracy: 0.9531\n",
            "batch: 200, train loss: 0.1407\n",
            "epoch: 12, train accuracy: 0.9844\n",
            "batch: 400, train loss: 0.0460\n",
            "epoch: 12, train accuracy: 0.9531\n",
            "batch: 600, train loss: 0.2066\n",
            "epoch: 12, validation accuracy: 0.7656\n",
            "batch: 0, validation loss: 1.3428\n",
            "....start of epoch: 13\n",
            ">>>>please wait....\n",
            "epoch: 13, train accuracy: 0.9375\n",
            "batch: 0, train loss: 0.1910\n",
            "epoch: 13, train accuracy: 0.9688\n",
            "batch: 200, train loss: 0.0773\n",
            "epoch: 13, train accuracy: 0.9531\n",
            "batch: 400, train loss: 0.1235\n",
            "epoch: 13, train accuracy: 0.9375\n",
            "batch: 600, train loss: 0.1523\n",
            "epoch: 13, validation accuracy: 0.8750\n",
            "batch: 0, validation loss: 0.5873\n",
            "....start of epoch: 14\n",
            ">>>>please wait....\n",
            "epoch: 14, train accuracy: 0.9531\n",
            "batch: 0, train loss: 0.1238\n",
            "epoch: 14, train accuracy: 0.9688\n",
            "batch: 200, train loss: 0.0539\n",
            "epoch: 14, train accuracy: 1.0000\n",
            "batch: 400, train loss: 0.0162\n",
            "epoch: 14, train accuracy: 1.0000\n",
            "batch: 600, train loss: 0.0143\n",
            "epoch: 14, validation accuracy: 0.7969\n",
            "batch: 0, validation loss: 1.0262\n",
            "....start of epoch: 15\n",
            ">>>>please wait....\n",
            "epoch: 15, train accuracy: 0.9531\n",
            "batch: 0, train loss: 0.1211\n",
            "epoch: 15, train accuracy: 0.9844\n",
            "batch: 200, train loss: 0.0558\n",
            "epoch: 15, train accuracy: 0.9844\n",
            "batch: 400, train loss: 0.1197\n",
            "epoch: 15, train accuracy: 0.9688\n",
            "batch: 600, train loss: 0.0405\n",
            "epoch: 15, validation accuracy: 0.8906\n",
            "batch: 0, validation loss: 0.5709\n",
            "....start of epoch: 16\n",
            ">>>>please wait....\n",
            "epoch: 16, train accuracy: 1.0000\n",
            "batch: 0, train loss: 0.0214\n",
            "epoch: 16, train accuracy: 0.9531\n",
            "batch: 200, train loss: 0.0689\n",
            "epoch: 16, train accuracy: 0.9844\n",
            "batch: 400, train loss: 0.0335\n",
            "epoch: 16, train accuracy: 0.9844\n",
            "batch: 600, train loss: 0.0352\n",
            "epoch: 16, validation accuracy: 0.8438\n",
            "batch: 0, validation loss: 0.9980\n",
            "....start of epoch: 17\n",
            ">>>>please wait....\n",
            "epoch: 17, train accuracy: 1.0000\n",
            "batch: 0, train loss: 0.0275\n",
            "epoch: 17, train accuracy: 0.9688\n",
            "batch: 200, train loss: 0.1126\n",
            "epoch: 17, train accuracy: 1.0000\n",
            "batch: 400, train loss: 0.0187\n",
            "epoch: 17, train accuracy: 0.9844\n",
            "batch: 600, train loss: 0.1384\n",
            "epoch: 17, validation accuracy: 0.8125\n",
            "batch: 0, validation loss: 0.8635\n",
            "....start of epoch: 18\n",
            ">>>>please wait....\n",
            "epoch: 18, train accuracy: 1.0000\n",
            "batch: 0, train loss: 0.0056\n",
            "epoch: 18, train accuracy: 0.9688\n",
            "batch: 200, train loss: 0.1441\n",
            "epoch: 18, train accuracy: 1.0000\n",
            "batch: 400, train loss: 0.0183\n",
            "epoch: 18, train accuracy: 0.9531\n",
            "batch: 600, train loss: 0.1504\n",
            "epoch: 18, validation accuracy: 0.8438\n",
            "batch: 0, validation loss: 0.7647\n",
            "....start of epoch: 19\n",
            ">>>>please wait....\n",
            "epoch: 19, train accuracy: 1.0000\n",
            "batch: 0, train loss: 0.0149\n",
            "epoch: 19, train accuracy: 0.9062\n",
            "batch: 200, train loss: 0.2379\n",
            "epoch: 19, train accuracy: 0.9844\n",
            "batch: 400, train loss: 0.0649\n",
            "epoch: 19, train accuracy: 1.0000\n",
            "batch: 600, train loss: 0.0103\n",
            "epoch: 19, validation accuracy: 0.8438\n",
            "batch: 0, validation loss: 0.7592\n",
            "....start of epoch: 20\n",
            ">>>>please wait....\n",
            "epoch: 20, train accuracy: 0.9688\n",
            "batch: 0, train loss: 0.0833\n",
            "epoch: 20, train accuracy: 0.9688\n",
            "batch: 200, train loss: 0.0597\n",
            "epoch: 20, train accuracy: 0.9844\n",
            "batch: 400, train loss: 0.0514\n",
            "epoch: 20, train accuracy: 0.9688\n",
            "batch: 600, train loss: 0.0502\n",
            "epoch: 20, validation accuracy: 0.8438\n",
            "batch: 0, validation loss: 0.8566\n",
            "....start of epoch: 21\n",
            ">>>>please wait....\n",
            "epoch: 21, train accuracy: 0.9844\n",
            "batch: 0, train loss: 0.0301\n",
            "epoch: 21, train accuracy: 0.9531\n",
            "batch: 200, train loss: 0.0778\n",
            "epoch: 21, train accuracy: 0.9531\n",
            "batch: 400, train loss: 0.0664\n",
            "epoch: 21, train accuracy: 1.0000\n",
            "batch: 600, train loss: 0.0168\n",
            "epoch: 21, validation accuracy: 0.8125\n",
            "batch: 0, validation loss: 0.6646\n",
            "....start of epoch: 22\n",
            ">>>>please wait....\n",
            "epoch: 22, train accuracy: 1.0000\n",
            "batch: 0, train loss: 0.0059\n",
            "epoch: 22, train accuracy: 1.0000\n",
            "batch: 200, train loss: 0.0203\n",
            "epoch: 22, train accuracy: 0.9844\n",
            "batch: 400, train loss: 0.0314\n",
            "epoch: 22, train accuracy: 0.9844\n",
            "batch: 600, train loss: 0.0244\n",
            "epoch: 22, validation accuracy: 0.8438\n",
            "batch: 0, validation loss: 1.0114\n",
            "....start of epoch: 23\n",
            ">>>>please wait....\n",
            "epoch: 23, train accuracy: 1.0000\n",
            "batch: 0, train loss: 0.0165\n",
            "epoch: 23, train accuracy: 1.0000\n",
            "batch: 200, train loss: 0.0062\n",
            "epoch: 23, train accuracy: 1.0000\n",
            "batch: 400, train loss: 0.0190\n",
            "epoch: 23, train accuracy: 1.0000\n",
            "batch: 600, train loss: 0.0133\n",
            "epoch: 23, validation accuracy: 0.8594\n",
            "batch: 0, validation loss: 0.9439\n",
            "....start of epoch: 24\n",
            ">>>>please wait....\n",
            "epoch: 24, train accuracy: 1.0000\n",
            "batch: 0, train loss: 0.0094\n",
            "epoch: 24, train accuracy: 0.9844\n",
            "batch: 200, train loss: 0.0338\n",
            "epoch: 24, train accuracy: 0.9688\n",
            "batch: 400, train loss: 0.0742\n",
            "epoch: 24, train accuracy: 1.0000\n",
            "batch: 600, train loss: 0.0283\n",
            "epoch: 24, validation accuracy: 0.8750\n",
            "batch: 0, validation loss: 0.6852\n",
            "....start of epoch: 25\n",
            ">>>>please wait....\n",
            "epoch: 25, train accuracy: 1.0000\n",
            "batch: 0, train loss: 0.0060\n",
            "epoch: 25, train accuracy: 0.9844\n",
            "batch: 200, train loss: 0.0876\n",
            "epoch: 25, train accuracy: 0.9688\n",
            "batch: 400, train loss: 0.0437\n",
            "epoch: 25, train accuracy: 1.0000\n",
            "batch: 600, train loss: 0.0143\n",
            "epoch: 25, validation accuracy: 0.8281\n",
            "batch: 0, validation loss: 0.8221\n",
            "....start of epoch: 26\n",
            ">>>>please wait....\n",
            "epoch: 26, train accuracy: 0.9844\n",
            "batch: 0, train loss: 0.1151\n",
            "epoch: 26, train accuracy: 1.0000\n",
            "batch: 200, train loss: 0.0011\n",
            "epoch: 26, train accuracy: 0.9688\n",
            "batch: 400, train loss: 0.0664\n",
            "epoch: 26, train accuracy: 0.9844\n",
            "batch: 600, train loss: 0.0359\n",
            "epoch: 26, validation accuracy: 0.8750\n",
            "batch: 0, validation loss: 1.1833\n",
            "....start of epoch: 27\n",
            ">>>>please wait....\n",
            "epoch: 27, train accuracy: 1.0000\n",
            "batch: 0, train loss: 0.0057\n",
            "epoch: 27, train accuracy: 1.0000\n",
            "batch: 200, train loss: 0.0024\n",
            "epoch: 27, train accuracy: 1.0000\n",
            "batch: 400, train loss: 0.0029\n",
            "epoch: 27, train accuracy: 1.0000\n",
            "batch: 600, train loss: 0.0062\n",
            "epoch: 27, validation accuracy: 0.8906\n",
            "batch: 0, validation loss: 0.8701\n",
            "....start of epoch: 28\n",
            ">>>>please wait....\n",
            "epoch: 28, train accuracy: 0.9844\n",
            "batch: 0, train loss: 0.0292\n",
            "epoch: 28, train accuracy: 0.9844\n",
            "batch: 200, train loss: 0.0251\n",
            "epoch: 28, train accuracy: 1.0000\n",
            "batch: 400, train loss: 0.0205\n",
            "epoch: 28, train accuracy: 1.0000\n",
            "batch: 600, train loss: 0.0071\n",
            "epoch: 28, validation accuracy: 0.8438\n",
            "batch: 0, validation loss: 1.1660\n",
            "....start of epoch: 29\n",
            ">>>>please wait....\n",
            "epoch: 29, train accuracy: 0.9688\n",
            "batch: 0, train loss: 0.0934\n",
            "epoch: 29, train accuracy: 0.9844\n",
            "batch: 200, train loss: 0.0556\n",
            "epoch: 29, train accuracy: 0.9688\n",
            "batch: 400, train loss: 0.0921\n",
            "epoch: 29, train accuracy: 1.0000\n",
            "batch: 600, train loss: 0.0166\n",
            "epoch: 29, validation accuracy: 0.8438\n",
            "batch: 0, validation loss: 1.0993\n",
            "....start of epoch: 30\n",
            ">>>>please wait....\n",
            "epoch: 30, train accuracy: 1.0000\n",
            "batch: 0, train loss: 0.0094\n",
            "epoch: 30, train accuracy: 0.9844\n",
            "batch: 200, train loss: 0.0286\n",
            "epoch: 30, train accuracy: 1.0000\n",
            "batch: 400, train loss: 0.0038\n",
            "epoch: 30, train accuracy: 0.9844\n",
            "batch: 600, train loss: 0.0495\n",
            "epoch: 30, validation accuracy: 0.8906\n",
            "batch: 0, validation loss: 0.5624\n",
            "\n",
            ">>> time elaplse for training and evaluation is: 0: 54: 18.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2CQ6ilUWuGL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}